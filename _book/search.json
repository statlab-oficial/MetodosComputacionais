[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M√©todos Computacionais",
    "section": "",
    "text": "Pref√°cio\nEste livro resulta de anos de experi√™ncia em sala de aula dos professores Ronald Targino, Rafael Braz, Juv√™ncio Nobre e Manoel Santos-Neto. Destina-se a apoiar os alunos da gradua√ß√£o em Estat√≠stica e do Programa de P√≥s-Gradua√ß√£o em Modelagem e M√©todos Quantitativos (PPGMMQ) do Departamento de Estat√≠stica e Matem√°tica Aplicada (DEMA) da Universidade Federal do Cear√° (UFC).\nAo longo dos cap√≠tulos, abordamos a gera√ß√£o de n√∫meros aleat√≥rios (discretos e cont√≠nuos); m√©todos de suaviza√ß√£o; simula√ß√£o estoc√°stica por invers√£o, rejei√ß√£o e composi√ß√£o, bem como m√©todos de reamostragem; m√©todos de aproxima√ß√£o e integra√ß√£o; quadratura Gaussiana, integra√ß√£o de Monte Carlo e quadratura adaptativa; m√©todos de Monte Carlo em sentido amplo; amostradores MCMC, com √™nfase em Gibbs e Metropolis‚ÄìHastings; otimiza√ß√£o num√©rica via Newton‚ÄìRaphson, Fisher scoring e quase-Newton, al√©m do algoritmo EM; Bootstrap e Jackknife; diagn√≥stico de converg√™ncia; e aspectos computacionais em problemas pr√°ticos, com foco em implementa√ß√£o eficiente, estabilidade num√©rica e reprodutibilidade dos resultados.\nEsperamos que este material sirva n√£o apenas como texto-base para as disciplinas Estat√≠stica Computacional (gradua√ß√£o em Estat√≠stica) e M√©todos Computacionais em Estat√≠stica (Mestrado-PPGMMQ), mas tamb√©m como suporte para aqueles que desejam programar com qualidade na √°rea de Estat√≠stica.",
    "crumbs": [
      "Pref√°cio"
    ]
  },
  {
    "objectID": "introducao.html",
    "href": "introducao.html",
    "title": "1¬† Introdu√ß√£o",
    "section": "",
    "text": "A simula√ß√£o tem um papel preponderante na estat√≠stica moderna, e suas vantagens no ensino de Estat√≠stica s√£o conhecidas h√° muito tempo. Em um de seus primeiros n√∫meros, o peri√≥dico Teaching Statistics publicou artigos que aludem precisamente a isso. Thomas e Moore (1980) afirmaram que ‚Äúa introdu√ß√£o do computador na sala de aula escolar trouxe uma nova t√©cnica para o ensino, a t√©cnica da simula√ß√£o‚Äù. Zieffler e Garfield (2007) e Tintle et al. (2015) discutem o papel e a import√¢ncia da aprendizagem baseada em simula√ß√£o no curr√≠culo de gradua√ß√£o em Estat√≠stica. No entanto, outros autores (por exemplo, Hodgson e Burke 2000) discutem alguns problemas que podem surgir ao ensinar uma disciplina por meio de simula√ß√£o, a saber, o desenvolvimento de certos equ√≠vocos na mente dos estudantes (Martins 2018).\nTodo estudante da Universidade Federal do Cear√° conhece a cena. √â hora do almo√ßo no Restaurante Universit√°rio (RU). A fila se alonga pelo p√°tio, colegas conversam, alguns reclamam da espera, outros aproveitam o tempo para revisar o conte√∫do da pr√≥xima prova. O tempo parece correr de forma diferente quando estamos na fila. Para alguns, s√£o apenas alguns minutos. Para outros, parece uma eternidade.\nAgora, pense um pouco. Quanto tempo, em m√©dia, um aluno passa esperando para se servir? Qual a chance de algu√©m que chega por volta das 12h30 esperar mais de vinte minutos? Por que em alguns dias a fila anda r√°pido e em outros parece n√£o ter fim?\nEssas perguntas podem parecer simples, mas abrem caminho para um universo fascinante. Elas revelam como a Probabilidade e a Estat√≠stica est√£o presentes em situa√ß√µes que vivemos todos os dias. A fila do RU n√£o √© apenas um detalhe da rotina estudantil. Ela √© um retrato de como os fen√¥menos aleat√≥rios acontecem ao nosso redor. Cada chegada de estudante, cada tempo de atendimento, cada varia√ß√£o de um dia para o outro forma um sistema din√¢mico que pode ser estudado e compreendido.\n√â esse o convite deste livro. Explorar como traduzir a realidade em modelos, como usar simula√ß√µes para observar padr√µes e como a Estat√≠stica pode ajudar a responder perguntas sobre o nosso cotidiano. Mais do que f√≥rmulas, ela √© uma maneira de olhar o mundo e encontrar nele sentido.\nAprender Estat√≠stica √© aprender a lidar com a incerteza. √â descobrir que at√© na fila do almo√ßo existe conhecimento escondido, esperando para ser revelado.\n\n\n\n\nHodgson, Ted, e Maurice Burke. 2000. ‚ÄúOn Simulation and the Teaching of Statistics‚Äù. Teaching Statistics 22 (3): 91‚Äì96. https://doi.org/10.1111/1467-9639.00033.\n\n\nMartins, Rui Manuel Da Costa. 2018. ‚ÄúLearning the Principles of Simulation Using the Birthday Problem‚Äù. Teaching Statistics 40 (3): 108‚Äì11. https://doi.org/10.1111/test.12164.\n\n\nThomas, F. H., e J. L. Moore. 1980. ‚ÄúCUSUM: Computer Simulation for Statistics Teaching‚Äù. Teaching Statistics 2 (1): 23‚Äì28. https://doi.org/10.1111/j.1467-9639.1980.tb00374.x.\n\n\nTintle, Nathan, Beth Chance, George Cobb, Soma Roy, Todd Swanson, e Jill VanderStoep. 2015. ‚ÄúCombating Anti-Statistical Thinking Using Simulation-Based Methods Throughout the Undergraduate Curriculum‚Äù. The American Statistician 69 (4): 362‚Äì70. https://doi.org/10.1080/00031305.2015.1081619.\n\n\nZieffler, Andrew, e Joan B. Garfield. 2007. ‚ÄúStudying the Role of Simulation in Developing Students‚Äô Statistical Reasoning‚Äù. Em Proceedings of the 56th Session of the International Statistical Institute (ISI). International Statistical Institute.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introdu√ß√£o</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2¬† Motiva√ß√£o",
    "section": "",
    "text": "Da teoria √† simula√ß√£o\nDo ponto de vista te√≥rico, a probabilidade de que todos os anivers√°rios sejam distintos entre \\(r\\) pessoas √©\n\\[\\Pr(\\text{todos distintos}) \\;=\\; \\prod_{i=1}^{r-1} \\frac{365-i}{365}\n\\;=\\; \\left(1 - \\frac{1}{365}\\right)\\!\\left(1 - \\frac{2}{365}\\right)\\!\\cdots\\!\\left(1 - \\frac{r-1}{365}\\right).\\]\nLogo, a probabilidade de pelo menos uma coincid√™ncia √©\n\\[p_r = 1 - \\Pr(\\text{todos distintos}).\\]\nEsse produto √© conceitualmente claro, mas fica pouco manej√°vel mentalmente para \\(k\\) moderados. √â aqui que a simula√ß√£o computacional pode entrar como aliada did√°tica e cient√≠fica.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Motiva√ß√£o</span>"
    ]
  },
  {
    "objectID": "intro.html#um-atalho-anal√≠tico-√∫til",
    "href": "intro.html#um-atalho-anal√≠tico-√∫til",
    "title": "2¬† Motiva√ß√£o",
    "section": "Um atalho anal√≠tico √∫til",
    "text": "Um atalho anal√≠tico √∫til\nO produto acima admite uma aproxima√ß√£o exponencial simples e acurada, obtida tomando logaritmo e usando a expans√£o para argumentos pequenos:\n\\[\\ln(1 - x) = -x + o(x), \\quad (x \\to 0). \\] Aplicando ao produto,\n\\[\n\\begin{aligned}\n\\ln\\!\\big(1 - p_r\\big)\n\\;&=\\;\n\\sum_{i=1}^{r-1} \\ln\\!\\left(1 - \\frac{i}{365}\\right)\\\\\n\\;&\\approx\\;\n-\\,\\sum_{i=1}^{r-1} \\frac{i}{365}\n\\;=\\;\n-\\,\\frac{1 + 2 + \\cdots + (r-1)}{365}\n\\;=\\;\n-\\,\\frac{r(r-1)}{2 \\cdot 365}.\\end{aligned}\\]\nExponentiando e isolando \\(p_r\\), obtemos a aproxima√ß√£o\n\\[\np_r \\;\\approx\\; 1 - \\exp\\!\\left\\{-\\,\\frac{r(r-1)}{730}\\right\\}.\n\\]\nEssa f√≥rmula tem tr√™s virtudes did√°ticas:\n\nClareza: exibe explicitamente o papel do n√∫mero de pares \\(\\binom{r}{2}\\).\nRapidez: permite c√°lculos aproximados para valores de \\(r\\) de interesse.\nBoas aproxima√ß√µes j√° para \\(r\\) na casa das dezenas.\n\n\n\n\n\n\n\nExemplo R√°pido\n\n\n\n\nPara 23 pessoas:\n\n\\[p_{23}^{(\\text{aprox})}\n  \\;=\\;\n  1 - \\exp\\!\\left\\{-\\frac{23\\cdot22}{730}\\right\\}\n  \\;=\\;\n  1 - \\exp\\!\\{-0.69315\\}\n  \\;\\approx\\; 0.500,\\]\nalinhando-se ao resultado cl√°ssico de que 23 pessoas j√° superam 50% de chance de coincid√™ncia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Motiva√ß√£o</span>"
    ]
  },
  {
    "objectID": "intro.html#o-papel-da-simula√ß√£o",
    "href": "intro.html#o-papel-da-simula√ß√£o",
    "title": "2¬† Motiva√ß√£o",
    "section": "O papel da simula√ß√£o",
    "text": "O papel da simula√ß√£o\nA simula√ß√£o estat√≠stica permite reproduzir o experimento de forma emp√≠rica: sorteamos aleatoriamente dias de anivers√°rio para os indiv√≠duos e verificamos se h√° repeti√ß√µes. Repetindo o processo milhares de vezes, obtemos uma estimativa para a probabilidade de coincid√™ncia.\nPor exemplo, em R:\n\nk &lt;- 23\nbirthdays &lt;- sample(1:365, k, replace = TRUE)\nany(duplicated(birthdays))\n\n[1] FALSE\n\n\nAo repetir esse procedimento muitas vezes (por exemplo, 10.000 simula√ß√µes), podemos estimar a propor√ß√£o de conjuntos com coincid√™ncia. Pela Lei dos Grandes N√∫meros, essa estimativa converge para o valor te√≥rico de aproximadamente 0,507 quando \\(k=23\\).\n\nset.seed(123) #reprodutibilidade\n\nk &lt;- 23\nB &lt;- 10000\n\nacertos &lt;- 0L\ni &lt;- 0L\n\nrepeat {\n  i &lt;- i + 1L\n  bdays &lt;- sample(1:365, k, replace = TRUE)\n  acertos &lt;- acertos + as.integer(any(duplicated(bdays)))\n  if (i &gt;= B) break\n}\n\np_hat &lt;- acertos / B\np_hat\n\n[1] 0.5073",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Motiva√ß√£o</span>"
    ]
  },
  {
    "objectID": "intro.html#atividade-problema-do-anivers√°rio-22-jogadores",
    "href": "intro.html#atividade-problema-do-anivers√°rio-22-jogadores",
    "title": "2¬† Motiva√ß√£o",
    "section": "Atividade: Problema do Anivers√°rio (22 jogadores)",
    "text": "Atividade: Problema do Anivers√°rio (22 jogadores)\nNesta motiva√ß√£o consideramos um exemplo discutido em Martins (2018) que √© o conhecido e amplamente divulgado problema do anivers√°rio (ver, por exemplo, Falk 2014). Martins (2018) segue o exemplo de Matthews e Stones (1998), considerando duas equipes de futebol e, portanto, coincid√™ncias de anivers√°rio entre 22 jogadores. Martins (2018) afirma que um resultado positivo importante dessa atividade √© a discuss√£o que surgir√° naturalmente entre os estudantes, com o professor atuando como mediador. Al√©m disso, os estudantes adoram jogos e a descoberta pr√°tica, e a simula√ß√£o facilita o engajamento nessas atividades, ao mesmo tempo que ilustra resultados que podem ser n√£o intuitivos, bem como teoria geral, como a Lei dos Grandes N√∫meros.\nAgora iremos considerar o seguinte problema:\n\n\n\n\n\n\nProblema\n\n\n\nEm uma partida de futebol, qual √© a probabilidade de que pelo menos dois dos 22 jogadores fa√ßam anivers√°rio no mesmo dia?\n\n\nEm um pais chamado de pa√≠s do futebol, o contexto √© proposital: o futebol √© popular e as probabilidades resultantes s√£o contraintuitivas. Antes de qualquer c√°lculo, considere as hip√≥teses: (i) todos os 365 dias do ano s√£o igualmente prov√°veis para qualquer anivers√°rio; (ii) as datas de anivers√°rio dos jogadores s√£o independentes entre si.\nObjetivos\n\nEstimar, via simula√ß√£o, a probabilidade de coincid√™ncia de anivers√°rios.\nRelacionar frequ√™ncia relativa, Lei dos Grandes N√∫meros e varia√ß√£o amostral.\nComparar o resultado exato e aproximado.\n\nHip√≥teses\n\n365 dias equiprov√°veis, datas independentes, ignorar bissexto/g√™meos.\n\nMateriais\n\nR (ou Posit Cloud), roteiro com comandos sample(), table(), mean().",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Motiva√ß√£o</span>"
    ]
  },
  {
    "objectID": "intro.html#exerc√≠cios",
    "href": "intro.html#exerc√≠cios",
    "title": "2¬† Motiva√ß√£o",
    "section": "Exerc√≠cios",
    "text": "Exerc√≠cios\n\nDeterminar o menor n√∫mero de pessoas que deve estar em uma sala para que se possa apostar, com mais de 50% de chance de ganhar, que entre elas existam pelo menos duas com o mesmo anivers√°rio.\nDeterminar o menor n√∫mero de outras pessoas que deve estar em uma sala com voc√™ para que se possa apostar, com mais de 50% de chance de ganhar, que pelo menos uma delas tenha o mesmo anivers√°rio que o seu.\n\n\n\n\n\nFalk, Ruma. 2014. ‚ÄúA Closer Look at the Notorious Birthday Coincidences‚Äù. Teaching Statistics 36 (2): 41‚Äì46. https://doi.org/10.1111/test.12014.\n\n\nMartins, Rui Manuel Da Costa. 2018. ‚ÄúLearning the Principles of Simulation Using the Birthday Problem‚Äù. Teaching Statistics 40 (3): 108‚Äì11. https://doi.org/10.1111/test.12164.\n\n\nMatthews, Robert, e Fiona Stones. 1998. ‚ÄúCoincidences: the truth is out there‚Äù. Teaching Statistics 20 (1): 17‚Äì19. https://doi.org/https://doi.org/10.1111/j.1467-9639.1998.tb00752.x.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Motiva√ß√£o</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3¬† N√∫meros Uniformes",
    "section": "",
    "text": "3.1 Gera√ß√£o de sequ√™ncias \\(U(0,1)\\)\nUma abordagem √© utilizar dispositivos f√≠sicos aleatorizadores, como m√°quinas que sorteiam n√∫meros de loteria, roletas ou circuitos eletr√¥nicos que produzem ‚Äúru√≠do aleat√≥rio‚Äù. Contudo, tais dispositivos apresentam desvantagens:\nUma forma simples de obter reprodutibilidade √© armazenar a sequ√™ncia em um dispositivo de mem√≥ria (HD, CD-ROM, livro). De fato, a RAND Corporation publicou A Million Random Digits with 100 000 Random Normal Deviates (1955). Entretanto, acessar armazenamento externo milhares ou milh√µes de vezes torna a simula√ß√£o lenta.\nAssim, a abordagem preferida √© gerar n√∫meros pseudoaleat√≥rios em tempo de execu√ß√£o, via recorr√™ncias determin√≠sticas sobre inteiros. Isso permite:\nEntretanto, a escolha inadequada da recorr√™ncia pode gerar sequ√™ncias com baixa qualidade estat√≠stica.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>N√∫meros Uniformes</span>"
    ]
  },
  {
    "objectID": "summary.html#gera√ß√£o-de-sequ√™ncias-u01",
    "href": "summary.html#gera√ß√£o-de-sequ√™ncias-u01",
    "title": "3¬† N√∫meros Uniformes",
    "section": "",
    "text": "Baixa velocidade e dificuldade de integra√ß√£o direta com computadores.\nNecessidade de reprodutibilidade da sequ√™ncia. Por exemplo, para verifica√ß√£o de c√≥digo ou compara√ß√£o de pol√≠ticas em um modelo de simula√ß√£o, usando a mesma sequ√™ncia para reduzir a vari√¢ncia da diferen√ßa entre resultados.\n\n\n\n\nGera√ß√£o r√°pida;\nElimina√ß√£o do problema de armazenamento;\nReprodutibilidade controlada.\n\n\n\nGeradores Congruenciais Lineares\nUm Gerador Congruencial Linear (LGC) produz uma sequ√™ncia de inteiros n√£o negativos \\(X_i\\), \\(i = 1, 2, \\ldots\\), por meio da rela√ß√£o de recorr√™ncia:\n\\[X_i = (a X_{i-1} + c) \\bmod m, \\quad i = 1, 2, \\ldots,\\] em que \\(a &gt; 0\\) √© o multiplicador, \\(X_0 \\ge 0\\) √© a semente (seed), \\(c \\ge 0\\) √© o incremento e \\(m &gt; 0\\) √© o m√≥dulo.\nOs valores \\(a, c, X_0\\) est√£o no intervalo \\([0, m-1]\\). O n√∫mero pseudoaleat√≥rio \\(R_i\\) √© obtido por:\n\\[R_i = \\frac{X_i}{m}, \\quad R_i \\in (0,1).\\]\nSe \\(m\\) for suficientemente grande, os valores discretos \\(0/m, 1/m, \\ldots, (m-1)/m\\) s√£o t√£o pr√≥ximos que \\(R_i\\) pode ser tratado como vari√°vel cont√≠nua.\n\nExemplo\nSeja o gerador:\n\\[X_i = (9 X_{i-1} + 3) \\bmod 24, \\quad i \\geq 1.\\]\nEscolhendo \\(X_0 = 3\\):\n\\[X_1 = (9 \\times 3 + 3) \\bmod 24 = 14\\]\n\\[X_2 = (9 \\times 14 + 3) \\bmod 24 = 1\\]\ne assim por diante.\nA sequ√™ncia \\(R_i = X_i / 24\\) gerada ter√° per√≠odo \\(\\ell = 24\\).\n\n\nImplementa√ß√£o em R\n\n# Fun√ß√£o LCG gen√©rica\nlcg &lt;- function(a, c, m, seed, n) {\n  x &lt;- numeric(n)\n  x[1] &lt;- seed\n  for (i in 2:n) {\n    x[i] &lt;- (a * x[i-1] + c) %% m\n  }\n  r &lt;- x / m\n  return(list(X = x, R = r))\n}\n\n# Exemplo com a = 9, c = 3, m = 24, seed = 3\nresultado &lt;- lcg(a = 9, c = 3, m = 24, seed = 3, n = 20)\nresultado$X\n\n [1]  3  6  9 12 15 18 21  0  3  6  9 12 15 18 21  0  3  6  9 12\n\nresultado$R\n\n [1] 0.125 0.250 0.375 0.500 0.625 0.750 0.875 0.000 0.125 0.250 0.375 0.500\n[13] 0.625 0.750 0.875 0.000 0.125 0.250 0.375 0.500\n\n\n\n\n\nGeradores Congruenciais Lineares Mistos\nNos LCGs mistos temos \\(c&gt;0\\). Uma escolha pr√°tica √© \\(m = 2^b\\), onde \\(b\\) √© o n√∫mero de bits utiliz√°vel para inteiros positivos na arquitetura/linguagem. Em muitos ambientes, inteiros usam 32 bits (um para o sinal), implicando \\(b=31\\) e intervalo \\([-2^{31}, 2^{31}-1]\\).\nQuando \\(m=2^b\\), obtemos per√≠odo completo (\\(\\ell=m\\)) se:\n\n\\(c\\) √© √≠mpar (garante \\(\\gcd(c,m)=1\\));\n\n\\(a-1\\) √© m√∫ltiplo de todos os fatores primos de \\(m\\) e tamb√©m de 4 (como \\(m\\) √© pot√™ncia de 2).\n\nEssa √© a raz√£o de geradores simples com \\(m=2^b\\), \\(c\\) √≠mpar e \\(a \\equiv 1 \\pmod 4\\) atingirem \\(\\ell=m\\).\n\n\nQuest√£o de estouro e aritm√©tica modular\nEm linguagens com inteiros limitados, calcular \\(aX_{i-1}+c\\) pode transbordar. Solu√ß√µes comuns:\n\nusar precis√£o estendida (64 bits) ou bibliotecas de inteiros grandes;\nempregar truques de aritm√©tica modular (como o m√©todo de Schrage) para evitar overflow;\ntrabalhar com m√≥dulo \\(m=2^b\\) e aproveitar o ‚Äúwrap‚Äù de bits.\n\nA seguir, implementamos LCG misto com \\(m=2^{31}\\), \\(a=906185749\\), \\(c=1\\). Par√¢metros com boas propriedades estat√≠sticas relatadas na literatura.\n\nImplementa√ß√£o em R (com seguran√ßa de overflow)\nPara garantir a corre√ß√£o do m√≥dulo com inteiros grandes, usaremos bit64 (inteiros de 64 bits) e normalizaremos para \\((0,1)\\).\n\n#if (!requireNamespace(\"bit64\", quietly = TRUE)) {\n#  install.packages(\"bit64\")\n#}\n\nlibrary(bit64)\n\nlcg_misto &lt;- function(n, seed = 3456L,\n                      a = 906185749L,\n                      c = 1L,\n                      m = bit64::as.integer64(2)^31) {\n  # Trabalha em integer64 para evitar perda de precis√£o\n  x &lt;- bit64::as.integer64(seed)\n  outX &lt;- bit64::integer64(n)\n  outR &lt;- numeric(n)\n  outX[1] &lt;- x\n  outR[1] &lt;- as.double(x) / as.double(m)\n  for (i in 2:n) {\n    x &lt;- (bit64::as.integer64(a) * x + bit64::as.integer64(c)) %% m\n    outX[i] &lt;- x\n    outR[i] &lt;- as.double(x) / as.double(m)\n  }\n  list(X = outX, R = outR)\n}\n\n# Exemplo: primeiros 5 n√∫meros com seed = 3456\nset.seed(NULL)\ng1 &lt;- lcg_misto(n = 5, seed = 3456L)\ng1$X\n\ninteger64\n[1] 3456       746789761  460230038  1591485775 1024426876\n\ng1$R\n\n[1] 1.609325e-06 3.477511e-01 2.143113e-01 7.410933e-01 4.770359e-01",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>N√∫meros Uniformes</span>"
    ]
  },
  {
    "objectID": "summary.html#geradores-congruenciais-lineares-multiplicativos",
    "href": "summary.html#geradores-congruenciais-lineares-multiplicativos",
    "title": "3¬† N√∫meros Uniformes",
    "section": "Geradores Congruenciais Lineares Multiplicativos",
    "text": "Geradores Congruenciais Lineares Multiplicativos\nNo caso multiplicativo, temos \\(c = 0\\), e a recorr√™ncia fica:\n\\[\nX_i = (a X_{i-1}) \\bmod m\n\\]\n\nCaracter√≠sticas e restri√ß√µes\n\nSe \\(X_i = 0\\) em algum passo, toda a sequ√™ncia futura ser√° zero ‚Äî portanto \\(X_0 \\neq 0\\).\nSe \\(a = 1\\), a sequ√™ncia √© constante ‚Äî tamb√©m deve ser evitado.\nO per√≠odo m√°ximo poss√≠vel √© \\(m - 1\\), e ele s√≥ √© atingido quando:\n\n\\(m\\) √© primo;\n\\(a\\) √© uma raiz primitiva m√≥dulo \\(m\\).\n\n\n\n\nDefini√ß√£o de raiz primitiva\nUm n√∫mero \\(a\\) √© raiz primitiva m√≥dulo \\(m\\) se seus poderes geram todos os inteiros n√£o nulos m√≥dulo \\(m\\).\nMatematicamente, \\(a\\) satisfaz:\n\\[\nm \\nmid a^{(m-1)/q} - 1, \\quad \\forall q \\text{ primo que divide } m-1\n\\]\nEsse tipo de gerador √© chamado Gerador de M√≥dulo Primo e Per√≠odo M√°ximo.\n\n\n\nExemplo de implementa√ß√£o em R\nA seguir, implementamos um gerador multiplicativo com m√≥dulo primo \\(m = 2^{31} - 1\\) (primo de Mersenne) e multiplicador \\(a = 630360016\\), conhecido por apresentar boas propriedades estat√≠sticas.\n\nif (!requireNamespace(\"gmp\", quietly = TRUE)) {\n  install.packages(\"gmp\")\n}\nlibrary(gmp)\n\nlcg_mult_primo &lt;- function(n, seed, a = 630360016, m = 2147483647) {\n  A &lt;- as.bigz(a); M &lt;- as.bigz(m)\n  x &lt;- as.bigz(seed)\n  X &lt;- integer(n); R &lt;- numeric(n)\n  for (i in seq_len(n)) {\n    X[i] &lt;- as.integer(x)    \n    R[i] &lt;- as.numeric(x) / m    \n    x &lt;- (A * x) %% M            \n  }\n  list(X = X, R = R)\n}\n\n# Exemplo: gerar 10 valores\ng2 &lt;- lcg_mult_primo(n = 10, seed = 12345L)\ng2$X\n\n [1]      12345 1461144439 1646755962  423395703 2041926374  720397004\n [7]  140279311  597861375  629442282  759842328\n\ng2$R\n\n [1] 5.748589e-06 6.803984e-01 7.668305e-01 1.971590e-01 9.508461e-01\n [6] 3.354610e-01 6.532264e-02 2.784009e-01 2.931069e-01 3.538292e-01",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>N√∫meros Uniformes</span>"
    ]
  },
  {
    "objectID": "summary.html#uso-de-n√∫meros-aleat√≥rios-na-avalia√ß√£o-de-integrais",
    "href": "summary.html#uso-de-n√∫meros-aleat√≥rios-na-avalia√ß√£o-de-integrais",
    "title": "3¬† N√∫meros Uniformes",
    "section": "3.2 Uso de N√∫meros Aleat√≥rios na Avalia√ß√£o de Integrais",
    "text": "3.2 Uso de N√∫meros Aleat√≥rios na Avalia√ß√£o de Integrais\nUma das primeiras aplica√ß√µes do uso de n√∫meros aleat√≥rios foi na resolu√ß√£o de integrais. Considere uma fun√ß√£o \\(g(x)\\) e suponha que desejamos calcular uma integral de interesse.\n\\[\\theta = \\int\\limits_{0}^{1}g(x) dx.\\]\nPara calcular o valor da integral, observe que, se \\(U\\) √© uma vari√°vel aleat√≥ria com distribui√ß√£o uniforme no intervalo \\((0, 1)\\), , ent√£o podemos reescrever a integral da seguinte forma:\n\\[\\theta = E[g(U)].\\] Se \\(U_1, U_2, \\dots,  U_n\\) s√£o vari√°veis aleat√≥rias independentes e uniformes em \\((0, 1)\\), ent√£o as vari√°veis \\(g(U_1), g(U_2), \\dots,  g(U_n)\\) s√£o indepedentes e identicamente distribu√≠das, todas com esperan√ßa igual \\(\\theta\\) (o valor da integral). Assim, pelo Teorema da Lei Forte dos Grandes N√∫meros, temos que, com probabilidade 1,\n\\[\\frac{1}{n}\\sum\\limits_{i=1}^{n}g(U_i) \\to \\theta \\quad \\text{quando} \\quad n \\to \\infty.\\]\nAssim, podemos aproximar o valor da integral gerando um grande n√∫mero de pontos aleat√≥rios \\(u_i\\) no intervalo \\((0, 1)\\) e tomando como estimativa a m√©dia dos valores \\(g(u_i)\\). Esse procedimento de aproxima√ß√£o de integrais √© conhecido como m√©todo de Monte Carlo.\nSe quisermos calcular uma integral mais geral, podemos aplicar a mesma ideia: transformar o problema em uma esperan√ßa matem√°tica e, em seguida, aproxim√°-la por meio de m√©dias amostrais obtidas a partir de n√∫meros aleat√≥rios. Considere:\n\\[\\theta = \\int\\limits_{a}^{b}g(x) dx.\\]\nSe quisermos calcular a integral em um intervalo gen√©rico \\((a, b)\\), fazemos a substitui√ß√£o\n\\[u = \\frac{x - a}{b-a}, \\quad du = \\frac{dx}{b-a},\\]\no que nos permite reescrev√™-la como\n\\[\\theta = \\int\\limits_{a}^{b}g(x) dx = (b-a)\\int\\limits_{0}^{1}g(a + (b-a)u) du.\\]\nDefinindo\n\\[h(u) = (b-a)g(a + (b-a)u),\\]\ntemos\n\\[\\theta = \\int\\limits_{0}^{1}h(u) du.\\]\nAssim, podemos aproximar \\(\\theta\\) gerando n√∫meros aleat√≥rios \\(u_1, u_2, \\dots, u_n \\sim U(0, 1)\\) e tomando como estimativa a m√©dia\n\\[\\theta \\approx \\frac{1}{n}\\sum\\limits_{i=1}^{n}h(u_i).\\] Agora, se nosso objetivo √© calcular a integral:\n\\[\\theta = \\int\\limits_{0}^{\\infty}g(x) dx.\\]\nFazendo a mudan√ßa de vari√°vel\n\\[u = \\frac{1}{x+1}, \\quad du = \\frac{-dx}{(x+1)^2} = -u^2 dx.\\]\nLogo,\n\\[dx = -\\frac{du}{u^2},\\]\ne a integral resultante √©\n\\[\\theta = \\int\\limits_{0}^{1}h(u) du,\\]\ncom\n\\[h(u) = \\frac{g\\left(\\frac{1}{u} - 1\\right)}{u^2}.\\]\nA utilidade de empregar n√∫meros aleat√≥rios para aproximar integrais torna-se ainda mais evidente no caso de integrais multidimensionais. Suponha que \\(g\\) seja uma fun√ß√£o com argumento \\(n\\)-dimensional e que estejamos interessados em calcular:\n\\[\\theta = \\int\\limits_{0}^{1}\\int\\limits_{0}^{1}\\dots\\int\\limits_{0}^{1} g(x_1, x_2, \\dots, x_n)dx_1 dx_2 \\dots dx_n.\\]\nObserve que \\(\\theta\\) pode ser expresso como o seguinte valor esperado:\n\\[\\theta = E[g(U_1, U_2, \\dots, U_n)],\\]\nem que \\(U_1, U_2, \\dots, U_n\\) s√£o vari√°veis aleat√≥ria independentes uniformente distribu√≠das no intervalo \\((0, 1)\\). Assim, se gerarmos \\(k\\) conjuntos independentes, cada um formado por \\(n\\) vari√°veis aleat√≥rias independentes com distribui√ß√£o uniforme em \\((0, 1)\\), ent√£o as vari√°veis\n\\[g(U_{i1}, U_{i2}, \\dots, U_{in}), \\quad i = 1, 2, \\dots, k,\\] ser√£o independentes e identicamente distribu√≠das, todas com esperan√ßa igual a \\(\\theta\\) (o valor da integral). Portanto, podemos aproximar \\(\\theta\\) por meio da m√©dia amostral:\n\\[\\theta \\approx \\frac{1}{k}\\sum\\limits_{i=1}^{k}g(U_{i1}, U_{i2}, \\dots, U_{in}).\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>N√∫meros Uniformes</span>"
    ]
  },
  {
    "objectID": "pseudo.html",
    "href": "pseudo.html",
    "title": "4¬† N√∫meros Pseudoaleat√≥rios",
    "section": "",
    "text": "4.1 Introdu√ß√£o",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>N√∫meros Pseudoaleat√≥rios</span>"
    ]
  },
  {
    "objectID": "pseudo.html#m√©todo-da-transforma√ß√£o-inversa",
    "href": "pseudo.html#m√©todo-da-transforma√ß√£o-inversa",
    "title": "4¬† N√∫meros Pseudoaleat√≥rios",
    "section": "4.2 M√©todo da Transforma√ß√£o Inversa",
    "text": "4.2 M√©todo da Transforma√ß√£o Inversa\nSuponha que desejamos gerar o valor de uma vari√°vel aleat√≥ria discreta \\(X\\) com fun√ß√£o de massa de probabilidade (f.m.p):\n\\[\\Pr(X = x_j) = p_j, \\quad j = 0, 1, \\dots, \\quad \\sum\\limits_j p_j = 1.\\]\n\n\n\n\n\n\nInteresse: Gerar valores da vari√°vel aleat√≥ria \\(X\\) com distribui√ß√£o:\n\n\n\n\n\n\n\\(X\\)\n\\(P(X=x_j)\\)\n\n\n\n\n\\(x_0\\)\n\\(p_0\\)\n\n\n\\(x_1\\)\n\\(p_1\\)\n\n\n\\(x_2\\)\n\\(p_2\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_j\\)\n\\(p_j\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\nPara realizar isso, gere um valor \\(u\\) de \\(U\\sim(0,1)\\) e atribua o valor \\(x_j\\), \\(j=0,1,\\ldots\\), a \\(X\\) conforme as condi√ß√µes abaixo:\n\\[X = \\begin{cases}\nx_0, & \\text{se } u &lt; p_0, \\\\\nx_1, & \\text{se } p_0 \\leq u &lt; p_0+p_1, \\\\\nx_2, & \\text{se } p_0+p_1 \\leq u &lt; p_0+p_1+p_2, \\\\\nx_3, & \\text{se } p_0+p_1+p_2 \\leq u &lt; p_0+p_1+p_2+p_3, \\\\\n\\vdots & \\\\\nx_j, & \\text{se } \\displaystyle \\sum_{i=0}^{j-1} p_i \\leq u &lt; \\sum_{i=0}^{j} p_i, \\\\\n\\vdots &\n\\end{cases}\\]\nComo, para \\(0 &lt; a &lt; b &lt; 1, \\Pr(a \\leq U &lt; b) = b-a\\), temos que:\n\\[\\Pr(X = x_j) = \\Pr\\left(\\sum\\limits_{i=0}^{j-1}p_i \\leq U &lt; \\sum\\limits_{i=0}^{j}p_i\\right) = p_j,\\] e, portanto, \\(X\\) possui a distribui√ß√£o desejada.\n\n\n\n\n\n\nObserva√ß√µes:\n\n\n\n\nAlgoritmo - O procedimento anterior pode ser escrito como:\n\nPasso 1. Gerar um valor \\(u \\sim U(0,1)\\).\nPasso 2. Se \\(u &lt; p_0\\), ent√£o \\(X = x_0\\); caso contr√°rio:\n\nse \\(u &lt; p_0 + p_1\\), ent√£o \\(X = x_1\\); caso contr√°rio:\n\nse \\(u &lt; p_0 + p_1 + p_2\\), ent√£o \\(X = x_2\\); caso contr√°rio:\n\\(\\vdots\\)\n\nPasso 3. Repetir os passos 1 e 2 \\(n\\) vezes, onde \\(n\\) √© o tamanho da amostra.\n\nSe os valores, \\(x_i, i \\geq 0\\), s√£o ordenados de modo que \\(x_0 &lt; x_1 &lt; \\dots,\\) e de denotamos \\(F\\) como fun√ß√£o de distribui√ß√£o de \\(X\\), ent√£o \\(F(x_j) = \\sum\\limits_{i=1}^{j}p_i\\). Assim,\n\n\\[X\\; \\text{ser√° igual a}\\; x_j\\; \\text{se}\\; F(x_{j-1}) &lt; U &lt; F(x_j).\\]\n\n\nEm outras palavras, ap√≥s gerar um n√∫mero aleat√≥rio \\(U\\), determinamos o valor de \\(X\\) encontrando o intervalo \\([F(x_{j-1}), F(x_j))\\) no qual \\(U\\) se encontra (ou, de forma equivalente, encontrando o inverso de \\(F(U)\\)). √â por essa raz√£o que o procedimento acima √© denominado m√©todo da transformada inversa discreta para gerar \\(X\\).\n\n\n\n\n\n\nNota\n\n\n\nO tempo necess√°rio para gerar uma vari√°vel aleat√≥ria discreta por esse m√©todo √© proporcional ao n√∫mero de intervalos que devem ser pesquisados. Por essa raz√£o, √†s vezes √© vantajoso considerar os poss√≠veis valores \\(x_j\\) de \\(X\\) em ordem decrescente das probabilidades \\(p_j\\).\n\n\n\nExemplo 1\nSimular \\(n\\) valores de \\(X\\) tal que \\(p_1 = 0.20\\), \\(p_2 = 0.15\\), \\(p_3 = 0.25\\), \\(p_4 = 0.40\\), em que \\(p_j = P(X=j)\\).\n\n\n\n\n\n\nAlgoritmo 1\n\n\n\nPasso 1. Gerar \\(n\\) valores \\(u \\sim U(0,1)\\).\nPasso 2. Para cada \\(u\\):\n\nSe \\(u &lt; 0.20\\), ent√£o \\(X = 1\\).\nCaso contr√°rio, se \\(u &lt; 0.35\\), ent√£o \\(X = 2\\).\nCaso contr√°rio, se \\(u &lt; 0.60\\), ent√£o \\(X = 3\\).\nCaso contr√°rio (\\(u \\geq 0.60\\)), ent√£o \\(X = 4\\).\n\nPasso 3. Repetir \\(n\\) vezes os passos 1 e 2.\n\n\nA seguir, s√£o apresentados dos c√≥digos escritos na linguagem R:\n\n# Proposta 1\nn &lt;- 10000\nx &lt;- 1:4 \np &lt;- c(0.20, 0.15, 0.25, 0.40)\npa &lt;- cumsum(p) # probabilidades acumuladas\na  &lt;- c() # vetor para a amostra gerada de X\n\nfor(i in 1:n){\n  u &lt;- runif(1)\n  if (u &lt; pa[1]) {\n    a[i] &lt;- x[1]\n  } else { \n      if (u &lt; pa[2]) {\n        a[i] &lt;- x[2]\n      } else { if (u &lt; pa[3]) {\n        a[i] &lt;- x[3]\n      } else { \n          a[i] &lt;- x[4]\n          }\n      }\n    }\n}\ntable(a)/n # tabela de propor√ß√µes\n\na\n     1      2      3      4 \n0.1974 0.1548 0.2539 0.3939 \n\n\n\n# Proposta 2\nn &lt;- 10000; \nx &lt;- 1:4; \np &lt;- c(0.20, 0.15, 0.25, 0.40)\npa &lt;- cumsum(p) # probabilidades acumuladas\na  &lt;- c() # vetor para a amostra gerada de X\n        \nfor(i in 1:n){\n  u = runif(1)\n  ifelse(u &lt; pa[1], a[i] &lt;- x[1], \n    ifelse(u &lt; pa[2], a[i] &lt;- x[2],\n      ifelse(u &lt; pa[3], a[i] &lt;- x[3], a[i] &lt;- x[4])))\n}\ntable(a)/n # tabela de propor√ß√µes\n\na\n     1      2      3      4 \n0.1919 0.1542 0.2509 0.4030 \n\n\n\n\n\n\n\n\nAlgoritmo 2\n\n\n\nPasso 1. Gerar um valor \\(u \\sim U(0,1)\\).\nPasso 2. Para cada \\(u\\):\n\nSe \\(u &lt; 0.40\\), ent√£o \\(X = 4\\);\nCaso contr√°rio, se \\(u &lt; 0.65\\), ent√£o \\(X = 3\\);\nCaso contr√°rio, se \\(u &lt; 0.85\\), ent√£o \\(X = 1\\);\nCaso contr√°rio \\(u \\geq 0.85\\), ent√£o \\(X = 2\\).\n\nPasso 3. Repetir \\(n\\) vezes os passos 1 e 2.\n\n\n\n\n\n\n\n\nüîé Observa√ß√£o:\n\n\n\nA proposta 2 usa a ordem decrescente dos \\(p_j\\) para otimizar a busca.\n\n\n\n\n\n\n\n\nAlgoritmo 3 (mudando as desigualdades)\n\n\n\nPasso 1. Gerar \\(u \\sim U(0,1)\\).\nPasso 2. Se \\(u \\leq 0.40\\), ent√£o \\(X = 4\\);\n\nsen√£o, se \\(u \\leq 0.65\\), ent√£o \\(X = 3\\);\nsen√£o, se \\(u \\leq 0.85\\), ent√£o \\(X = 1\\);\nsen√£o (\\(u &gt; 0.85\\)), ent√£o \\(X = 2\\).\n\nPasso 3. Repetir \\(n\\) vezes os passos 1‚Äì2.\n\n\n\n\n\n\n\n\nüîé Observa√ß√£o:\n\n\n\nCom \\(u \\sim U(0,1)\\) √© cont√≠nua, usar \\(&lt;\\) ou \\(\\leq\\) √© equivalente em termos de distribui√ß√£o (a probabilidade de \\(u\\) cair exatamente no ponto de corte √© zero). Em implementa√ß√£o num√©rica, essa escolha apenas define para onde v√£o rar√≠ssimos empates nos pontos cortes.\n\n\n\n\nInversa Generalizada\nSeja \\(F\\) uma fun√ß√£o de distribui√ß√£o qualquer. A inversa generalizada, denotada por \\(F^{-1}\\), √© definida da seguinte forma: \\[ F^{-1}(u) = \\mbox{inf}\\{x \\in \\mathbb{R}: F(x)\\geq u \\}\\]\n\nTanto a inversa de \\(F\\) como a inversa generalizada est√£o sendo denotadas por \\(F^{-1}\\).\nSe a inversa de \\(F\\) existe no sentido usual, ela coincide com a inversa generalizada.\nNote que, se \\(F\\) for estritamente crescente e cont√≠nua, ent√£o \\(\\forall x \\in \\mathbb{R}\\) existir√° apenas um \\(u\\) tal que \\(F(x) = u\\).\nA proposi√ß√£o seguinte d√° base para as simula√ß√µes de v.a. discretas.\n\nSeja \\(F\\) uma fun√ß√£o de distribui√ß√£o qualquer e \\(F^{-1}\\) sua inversa generalizada. Temos:\n\\[F^{-1}(u) \\leq x \\mbox{ se, e somente se, } u \\leq F(x)\\]\nSe voc√™ deseja simular valores de \\(X\\) com distribui√ß√£o uniforme discreta:\n\\[P(X=j)=1/k, \\, \\,\\, j=1,2,3,\\ldots, k.\\]\nBasta utilizar a Proposi√ß√µes 1 ou a Proposi√ß√£o 2.\n\n\n\n\n\n\nProposi√ß√£o 1:\n\n\n\nGere \\(U\\sim U(0,1)\\). Defina \\(X=j\\) se\n\\[\\frac{j-1}{k}\\leq U &lt; \\frac{j}{k},\\] ou, equivalentemente,\n\\[(j-1)\\leq kU &lt; j.\\]\n\n\n\n\n\n\n\n\nProposi√ß√£o 2:\n\n\n\nGere \\(U\\sim U(0,1)\\). Defina \\(X=\\mbox{Int}(kU)+1\\), em que \\(\\textrm{Int}(x)\\) √© a parte inteira de \\(x\\).\n\n\n\n\nExemplo 2\nPara simular valores de \\(X\\) com \\(P(X=j)=1/10, \\, \\,\\, j=1,2,3,\\ldots, 10\\), voc√™ pode serguir o procedimento a seguir:\n\n\n\n\n\n\n\n\n\n\n\n\\(j-1\\)\n\\(j\\)\n\\(U\\)\n\\(kU\\)\n\\(X=j\\)\n\\(X=\\mathrm{Int}(kU)+1\\)\n\n\n\n\n0\n1\n0,01\n0,1\n1\n1\n\n\n1\n2\n0,31\n3,1\n4\n4\n\n\n2\n3\n0,53\n5,3\n6\n6\n\n\n3\n4\n0,92\n9,2\n10\n10\n\n\n4\n5\n0,45\n4,5\n5\n5\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n9\n10\n0,74\n7,4\n8\n8\n\n\n\n\nExerc√≠cios\n\nGerar uma permuta√ß√£o dos n√∫meros \\(1,2,3, \\ldots, n\\), considerando todas as \\(n!\\) poss√≠veis permuta√ß√µes igualmente prov√°veis (exemplo 4b).\nGerar valores de \\(X\\sim \\mbox{Geom√©trica}(p)\\) (exemplo 4d).\nImplementar o algoritmo para gerar valores de \\(X\\sim \\mbox{Poisson}(\\lambda)\\) (se√ß√£o 4.2).\nImplementar o algoritmo para gerar valores de \\(X\\sim \\mbox{Binomial}(n,p)\\) (se√ß√£o 4.2).\n\n\n\n\nExemplo 3\nNosso objetivo agora √© gerar valores de \\(X\\sim \\mbox{Poisson}(\\lambda)\\) com:\n\\[p_i=P(X=i)=\\dfrac{e^{-\\lambda}\\lambda^i}{i!}, \\quad i=0,1,2,\\ldots. \\] ::: {.callout-important} ## üîé Identidade importante:\n\\[p_{i+1} = \\dfrac{\\lambda}{i+1}p_i, \\quad i\\geq 0.\\] :::\nUma forma bastante utilizada para gerar valores de uma vari√°vel aleat√≥ria \\(X\\sim \\mbox{Poisson}(\\lambda)\\) √© por meio de algoritmos de simula√ß√£o baseados na identidade acima.\n\n\n\n\n\n\nAlgoritmo 4\n\n\n\nPasso 1. Gerar um n√∫mero aleat√≥rio \\(u \\sim U(0,1)\\).\nPasso 2. Inicializar \\(i = 0\\), \\(p = e^{-\\lambda}\\), \\(F = p\\).\nPasso 3. Se \\(u &lt; F\\), ent√£o definir \\(X = i\\) e parar.\nPasso 4. Caso contr√°rio:\n\natualizar \\(i \\leftarrow i+1\\),\natualizar \\(p \\leftarrow \\frac{\\lambda}{i}\\,p\\),\natualizar \\(F \\leftarrow F + p\\),\nvoltar ao passo 3.\n\n\n\n\nExerc√≠cio\nComplete o c√≥digo R abaixo:\n\nN = 10^5 # tamanho da amostra\nL = 3  # lambda\nx = c()\nfor (j in 1:N){\n  u = runif(1)\n  i = 0; p = exp(-L); F = p\n  aceito = \"n√£o\"\n  while (aceito != \"sim\"){\n    if(u &lt; F) {\n       ...\n     } else {\n        ...\n      }\n  }\n}\n\n\n\n\nExemplo 4\nPor fim, iremos gerar valores da vari√°vel aleat√≥rio \\(X\\), \\(X\\sim \\mbox{Binomial}(n,p)\\), com f.m.p dada por:\n\\[P(X=i)=\\dfrac{n!}{i!(n-i)!}p^i(1-p)^{n-i}, \\quad i=0,1,2,\\ldots,n.\\]\n\n\n\n\n\n\nüîé Identidade importante::\n\n\n\n\\[P(X=i+1) = \\dfrac{n-i}{i+1}\\dfrac{p}{1-p}P(X=i).\\]\n\n\nA partir do resultado acima e do m√©todo da transforma√ß√£o inversa podemos escrever o seguinte algoritmo:\n\n\n\n\n\n\nAlgoritmo 5\n\n\n\nPasso 1. Gerar um n√∫mero aleat√≥rio \\(u \\sim U(0,1)\\).\nPasso 2. Calcular \\(k = \\dfrac{p}{1-p}\\), inicializar \\(i = 0\\), \\(p_r = (1-p)^n\\), \\(F = p_r\\).\nPasso 3. Se \\(u &lt; F\\), definir \\(X = i\\) e parar.\nPasso 4. Caso contr√°rio:\n\natualizar \\(p_r \\leftarrow \\dfrac{n-i}{i+1}\\,k\\,p_r\\),\natualizar \\(F \\leftarrow F + p_r\\),\natualizar \\(i \\leftarrow i+1\\),\nvoltar ao passo 3.\n\n\n\n\n\nExerc√≠cio 5\nEscreve um c√≥digo na linguagem R baseado no Algoritmo 5. Utilize alguma ferramenta gr√°fica para verificar a coer√™ncia dos resultados.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>N√∫meros Pseudoaleat√≥rios</span>"
    ]
  },
  {
    "objectID": "pseudo.html#m√©todo-da-aceita√ß√£o-rejei√ß√£o",
    "href": "pseudo.html#m√©todo-da-aceita√ß√£o-rejei√ß√£o",
    "title": "4¬† N√∫meros Pseudoaleat√≥rios",
    "section": "4.3 M√©todo da Aceita√ß√£o-Rejei√ß√£o",
    "text": "4.3 M√©todo da Aceita√ß√£o-Rejei√ß√£o\nSuponha que haja interesse em simular valores de uma v.a. \\(X\\) e que n√£o seja poss√≠vel inverter a sua fun√ß√£o de distribui√ß√£o ou n√£o dispomos de um m√©todo para gerar dessa vari√°vel aleat√≥ria. Entretanto, sabemos como simular de uma outra v.a. \\(Y\\) e que √© poss√≠vel estabelecer uma rela√ß√£o entre as probabilidades associadas √†s duas vari√°veis aleat√≥rias (\\(X\\) e \\(Y\\)) de tal modo que valores de \\(Y\\) possam ser admitidos como valores de \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\\(p_x\\)\n0,11\n0,12\n0,09\n0,08\n0,12\n0,10\n0,09\n0,09\n0,10\n0,10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(y\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\\(q_y\\)\n0,10\n0,10\n0,10\n0,10\n0,10\n0,10\n0,10\n0,10\n0,10\n0,10\n\n\n\nContexto:\n\nPodemos: simular valores de \\(Y\\) com f.p. \\(q_j=P(Y=j), j\\geq 0\\).\nQueremos: simular valores de \\(X\\) com f.p. \\(p_j=P(X=j), j\\geq 0\\).\nProposta: simular valor \\(y\\) de \\(Y\\) e aceitar esse valor para \\(X\\) com probabilidade proporcional a \\(\\dfrac{p_y}{q_y}\\).\n\n\n\n\n\n\n\nAlgoritmo\n\n\n\nPasso 1. Simular \\(y\\) de \\(Y\\) com f.m.p. \\(q_y\\).\nPasso 2. Gerar \\(u \\sim U(0,1)\\).\nPasso 3. Se \\(u &lt; \\dfrac{p_y}{c \\, q_y}\\), ent√£o aceite \\(X = y\\). Caso contr√°rio, rejeite e n√£o atribua valor a \\(X\\).\nPasso 4. Repetir os passos 1‚Äì3 at√© obter o tamanho de amostra desejado.\n\n\nO algoritmo aceita√ß√£o-rejei√ß√£o gera uma v.a. \\(X\\) tal que \\(P(X=j)=p_j\\), \\(j=0,1,2,\\ldots\\). Al√©m disso, o n√∫mero de itera√ß√µes que o algoritmo necessita para obter \\(X\\) √© uma v.a. geom√©trica com m√©dia \\(c\\).\n\n\n\n\n\n\nProva do Teorema\n\n\n\n\nEm uma itera√ß√£o, determinar a probabilidade de gerar e ser aceito o valor \\(j\\): \\[P(Y=j,aceitar)= P(Y=y).P(aceitar/Y=j)= q_j.\\dfrac{pj}{cq_j} = \\dfrac{p_j}{c}\\]\nCalcular a probabilidade de aceitar um valor \\(j\\) gerado: \\[P(aceitar)= \\sum_j P(Y=j,aceitar)=\\sum_j \\dfrac{pj}{c} = \\dfrac{1}{c}\\]\nComo cada itera√ß√£o independentemente resulta um valor aceit√°vel com probabilidade \\(\\frac{1}{c}\\), o n√∫mero de itera√ß√µes necess√°rias segue uma geom√©trica de m√©dia \\(c\\). Portanto, \\[P(X=j)=\\! \\sum_n P(\\mbox{\\textit{j aceito na itera√ß√£o n}}) = \\sum_n \\left(\\!1\\!-\\!\\dfrac{1}{c}\\!\\right)^{\\!\\!n-1}\\!\\!.\\dfrac{p_j}{c}=p_j\\].\n\n\n\n\n\n\n\n\n\nObserva√ß√µes\n\n\n\n\nA constante \\(c\\) est√° relacionada com o n√∫mero de intera√ß√µes necess√°rias at√© a aceita√ß√£o de um valor de \\(Y\\) para \\(X\\).\nO valor \\(c\\) deve ser o menor poss√≠vel.\nO valor de \\(c\\) ser√° o \\(\\mbox{max}\\left\\{\\dfrac{p_y}{q_y}\\right\\}\\).\n\n\n\n\n\n\n\n\n\nNota\n\n\n\n\\[u&lt;\\dfrac{p_y}{c.q_y} \\ \\ \\ \\ \\mbox{e}  \\ \\ \\ \\  P(U&lt;\\dfrac{p_y}{c.q_y})= \\dfrac{p_y}{c.q_y} \\] \\[\\Downarrow\\] \\[\\dfrac{p_y}{c.q_y}\\leq 1 \\ \\ \\ \\ \\mbox{para todo} \\ \\ y \\ \\ \\mbox{tal que} \\ \\ p_y&gt;0\\] \\[\\Downarrow\\] \\[c=\\mbox{max}\\left\\{\\dfrac{p_y}{q_y}\\right\\}\\]\n\n\n\nExemplo 1\nGerar um valor da vari√°vel aleat√≥ria \\(X\\) com f.m.p.:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(j\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\\(p_j\\)\n0,11\n0,12\n0,09\n0,08\n0,12\n0,10\n0,09\n0,09\n0,10\n0,10\n\n\n\nConsiderando que sabemos gerar de uma v.a. uniforme discreta, assumiremos \\(Y\\) com distribui√ß√£o\n\\[P(Y=j)=q_j=\\dfrac{1}{10}; \\quad j=1,2, \\ldots, 10.\\]\nA constante \\(c\\) ser√° determinada por \\(c=\\mbox{max}\\left\\{\\dfrac{p_j}{q_j}\\right\\} =\\dfrac{0,\\!12}{0,\\!10}=1,\\!2.\\)\n\n\n\n\n\n\nAlgoritmo\n\n\n\n\nSimular \\(y\\) de \\(Y\\): gere \\(u_1 \\sim U(0,1)\\) e fa√ßa \\(y = \\mathrm{Int}(10u_1)+1\\).\nGerar um segundo n√∫mero aleat√≥rio \\(u_2\\).\nSe \\(u_2 &lt; \\dfrac{p_y}{0.12}\\), fa√ßa \\(X=y\\) e pare. Caso contr√°rio, retorne ao passo 1.\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSuponha \\(y=1\\). Ent√£o, se \\(u_2&lt;\\dfrac{p_1}{0,\\!12}=\\dfrac{0,\\!11}{0,\\!12}=0,\\!9167\\), faremos \\(X=1\\). Isto √©, assumiremos que o valor 1 gerado √© plaus√≠vel de ser da distribui√ß√£o de \\(X\\). O gr√°fico a seguir ilustra esse processo.\n\n\n\n#set.seed(20252)\npseudo_x &lt;- NULL\nfor(i in seq_len(1000)){\n  \npj &lt;- c(0.11, 0.12, 0.09, 0.08, 0.12, 0.10, 0.09, 0.09, 0.10, 0.10)\n\nu1 &lt;- runif(1)\ny &lt;- floor(10*u1) + 1\n\n\nrepeat{\nu2 &lt;- runif(1)\nx &lt;- y\n\nif(u2 &lt; pj[y]/0.12) break\n}\n\npseudo_x[i] &lt;- x\n}\n\nround(prop.table(table(pseudo_x)),2)\n\npseudo_x\n   1    2    3    4    5    6    7    8    9   10 \n0.10 0.11 0.09 0.09 0.11 0.09 0.10 0.10 0.11 0.09 \n\n\nExiste algum problema com os resultados acima? Se sim, fa√ßa a corre√ß√£o do c√≥digo e verifique graficamente a coer√™ncia dos resultados.\n\n\n\n\n\n\nObserva√ß√µes\n\n\n\n\n\\(91,\\!67\\%\\) de todos os valores 1 gerados de \\(Y\\) ser√£o aceitos\n\\(100\\%\\) dos valores 2 gerados de \\(Y\\) ser√£o aceitos\n\\(75\\%\\) dos valores 3 gerados de \\(Y\\) ser√£o aceitos\nQualquer valor da constante \\(c\\) inferior a \\(1,\\!2\\) impossibilita gerar a distribui√ß√£o de \\(X\\)\nQualquer valor da constante \\(c\\) superior a \\(1,\\!2\\) tornaria o processo mais lento para a obte√ß√£o da amostra\n\n\n\n\n\nExerc√≠cios\n\nGere n√∫meros pseudoaleat√≥rios de \\(X\\) considerando \\(c = 2,4\\).\nCompare com os resultados obtidos no Exemplo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>N√∫meros Pseudoaleat√≥rios</span>"
    ]
  },
  {
    "objectID": "pseudo.html#m√©todo-da-composi√ß√£o",
    "href": "pseudo.html#m√©todo-da-composi√ß√£o",
    "title": "4¬† N√∫meros Pseudoaleat√≥rios",
    "section": "4.4 M√©todo da Composi√ß√£o",
    "text": "4.4 M√©todo da Composi√ß√£o\nSuponha que tenhamos um m√©todo eficiente para simular o valor de uma vari√°vel aleat√≥ria com f.m.p. \\({p_j, ; j \\geq 0}\\) ou \\({q_j, ; j \\geq 0}\\), e que desejamos simular a vari√°vel aleat√≥ria \\(X\\) com f.m.p.:\n\\[\\Pr(X = j) = \\alpha p_j + (1 - \\alpha)q_j, \\quad j \\geq 0, 0 &lt; a &lt; 1.\\] Se \\(X_1 \\sim {p_j}\\) e \\(X_2 \\sim {q_j}\\), ent√£o podemos definir\n\\[X =\n\\begin{cases}\nX_1, & \\text{com probabilidade } a, \\\\\nX_2, & \\text{com probabilidade } 1-a ,\n\\end{cases}.\\]\nAssim, \\(X\\) ter√° exatamente a fun√ß√£o de probabilidade acima.\n\nExemplo\nSuponha que desejamos gerar o valor de uma vari√°vel aleat√≥ria \\(X\\) tal que\n\\[p_j = \\Pr(X = j) =  \\begin{cases}\n0.05, & \\text{para}\\quad j = 1, 2, 3, 4, 5, \\\\\n0.15, & \\text{para}\\quad  j = 6, 7, 8, 9, 10,\n\\end{cases}\\]\nNote que \\(p_j = 0.5 \\times p_j^{(1)} + 0.5 \\times p_j^{(2)},\\) em que\n\\[p_j^{(1)} = 0.1, \\quad j = 1, \\dots, 10 \\quad \\text{e} \\quad p_j^{(2)} = \\begin{cases}\n0, & \\text{para}\\quad j = 1, 2, 3, 4, 5, \\\\\n0.2, & \\text{para}\\quad  j = 6, 7, 8, 9, 10,\n\\end{cases}\\]\npodemos realizar essa simula√ß√£o gerando primeiro um n√∫mero aleat√≥rio \\(U \\sim U(0,1)\\) e ent√£o:\n\nSe \\(U &lt; 0.5\\), gerar \\(X\\) de uma uniforme discreta sobre \\(\\{1,2,\\dots,10\\}\\).\nCaso contr√°rio (\\(U \\geq 0.5\\)), gerar \\(X\\) de uma uniforme discreta sobre \\(\\{6,7,8,9,10\\}\\).\n\n\n\n\n\n\n\nAlgoritmo\n\n\n\nPasso 1. Gerar \\(U_1 \\sim U(0,1)\\).\nPasso 2. Gerar \\(U_2 \\sim U(0,1)\\).\nPasso 3. Se \\(U_1 &lt; 0.5\\), definir \\(X = \\mathrm{Int}(10U_1)+1\\). Caso contr√°rio, definir \\(X = \\mathrm{Int}(5U_2)+6\\).\n\n\nSe \\(F_i\\), \\(i=1,\\dots,n\\) s√£o fun√ß√µes de distribui√ß√£o e \\(\\alpha_i\\), \\(i=1,\\dots,n\\) s√£o n√∫meros n√£o negativos cuja soma √© 1, ent√£o a fun√ß√£o de distribui√ß√£o:\n\\[\nF(x) = \\sum_{i=1}^n \\alpha_i F_i(x),\n\\]\n√© uma mistura, ou uma composi√ß√£o, das fun√ß√µes de distribui√ß√£o \\(F_i\\), \\(i = 1, \\dots, n\\).\nUma maneira de simular a partir de \\(F\\) √© primeiro simular uma vari√°vel aleat√≥ria \\(I\\), igual a \\(i\\) com probabilidade \\(\\alpha_i\\), \\(i=1,\\dots,n\\), e ent√£o simular a partir da distribui√ß√£o \\(F_I\\) (Isto √©, se o valor simulado de \\(I\\) for \\(I = j\\), ent√£o a segunda simula√ß√£o √© feita a partir de \\(F_j\\)). Essa abordagem para simular de \\(F\\) √© frequentemente chamada de m√©todo de composi√ß√£o.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>N√∫meros Pseudoaleat√≥rios</span>"
    ]
  },
  {
    "objectID": "otimizaca.html",
    "href": "otimizaca.html",
    "title": "5¬† Otimiza√ß√£o Num√©rica",
    "section": "",
    "text": "5.1 M√©todo de Newton",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Otimiza√ß√£o Num√©rica</span>"
    ]
  },
  {
    "objectID": "otimizaca.html#m√©todo-de-newton-raphson",
    "href": "otimizaca.html#m√©todo-de-newton-raphson",
    "title": "5¬† Otimiza√ß√£o Num√©rica",
    "section": "5.2 M√©todo de Newton-Raphson",
    "text": "5.2 M√©todo de Newton-Raphson",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Otimiza√ß√£o Num√©rica</span>"
    ]
  },
  {
    "objectID": "otimizaca.html#m√©todo-escore-de-fisher",
    "href": "otimizaca.html#m√©todo-escore-de-fisher",
    "title": "5¬† Otimiza√ß√£o Num√©rica",
    "section": "5.3 M√©todo Escore de Fisher",
    "text": "5.3 M√©todo Escore de Fisher",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Otimiza√ß√£o Num√©rica</span>"
    ]
  },
  {
    "objectID": "otimizaca.html#m√©todo-bfgs",
    "href": "otimizaca.html#m√©todo-bfgs",
    "title": "5¬† Otimiza√ß√£o Num√©rica",
    "section": "5.4 M√©todo BFGS",
    "text": "5.4 M√©todo BFGS",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Otimiza√ß√£o Num√©rica</span>"
    ]
  },
  {
    "objectID": "boot.html",
    "href": "boot.html",
    "title": "6¬† M√©todos de Reamostragem",
    "section": "",
    "text": "6.1 Bootstrap",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>M√©todos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#bootstrap",
    "href": "boot.html#bootstrap",
    "title": "6¬† M√©todos de Reamostragem",
    "section": "",
    "text": "6.1.1 Introdu√ß√£o\nBootstrap √© um m√©todo (computacional) de reamostragem baseado em subamostras de uma amostra observada, sendo introduzido por Efron (1979). Pode ser utilizado com o prop√≥sito de estimar erros padr√£o, vi√©s de estimadores, construir intervalos de confian√ßa, testes de hip√≥teses, entre outros. Pode ser utilizando sob duas abordagens: param√©trica e n√£o param√©trica. A abordagem param√©trica exige um modelo estat√≠stica, enquanto que na abordagem n√£o param√©trica n√£o h√° suposi√ß√£o de modelo estat√≠stico; toma-se por base uma distribui√ß√£o emp√≠rica que atribui probabilidade \\(1/n\\) para cada um dos \\(n\\) elementos da amostra. Algumas refer√™ncias importantes neste tema s√£o: Efron (1979), Wu (1986), Fisher \\(\\&\\) Hall (1989), Fredman (1986), Efron \\(\\&\\) Tibshirani (1993), Horowitz (1997), Davison \\(\\&\\) Hinkley (1997).\n\n\n6.1.2 Acur√°ria da m√©dia amostral\nExperimento com 16 ratos, divididos em dois grupos: um grupo recebeu o tratamento e um outro grupo n√£o recebeu o tratamento (controle). O tempo de sobreviv√™ncia (dias) √© apresentado para cada um dos ratos. O tratamento prolonga a vida?\n\n\n\n\n\n\n\n\n\n\n\n\ngrupo\ntempo 1\ntempo 2\ntempo 3\nn\nm√©dia\n\\(\\widehat{ep}\\)(m√©dia)\n\n\n\n\ntratamento (X)\n94\n197\n16\n7\n86,86\n25,24\n\n\n\n38\n99\n141\n\n\n\n\n\n\n23\n\n\n\n\n\n\n\ncontrole (Y)\n52\n104\n146\n9\n56,22\n14,14\n\n\n\n10\n51\n30\n\n\n\n\n\n\n40\n27\n46\n\n\n\n\n\ndiferen√ßa\n\n\n\n\n30,63\n28,93\n\n\n\nAlguns pontos importantes s√£o:\n\nA resposta √† pergunta depender√° de qu√£o acurado(s) √©(s√£o) o(s) estimador(es).\nO erro padr√£o √© uma medida (muito usual) de acur√°cia de estimador.\nerro padr√£o estimado para a m√©dia amostral \\(\\bar{X}\\) \\[\\widehat{ep}(\\bar{X})=\\sqrt{\\dfrac{s^2}{n}},\\] em que \\(s^2=\\sum_{i=1}^n(x_i-\\bar{x})^2/(n-1)\\).\nerro padr√£o de qualquer estimador √© definido pela raiz quadrada de sua vari√¢ncia\n\nAl√©m disso, temos que:\n\nerro padr√£o pequeno \\(\\longrightarrow\\) acur√°cia alta\nerro padr√£o grande \\(\\longrightarrow\\) acur√°cia baixa\nacur√°cia alta (baixa) indica que o estimador apresenta valores pr√≥ximos (distantes) ao seu valor esperado\nespera-se que 68\\(\\%\\) dos valores do estimador estejam a menos de um erro padr√£o do seu valor esperado, e 95\\(\\%\\), a menos de dois erros padr√µes\nerro padr√£o da diferen√ßa \\((\\bar{X}-\\bar{Y})\\): \\[28.93=\\sqrt{25.24^2+14.14^2}.\\]\n\nResposta √† pergunta: a diferen√ßa observada 30.63 √© somente 30.63/28.93=1.05 erros padr√µes (estimados) maior que zero, indicando um resultado n√£o significativo, ou seja, o tratamento n√£o aumenta o tempo m√©dio de vida (considerando a teoria dos testes de hip√≥teses).\nO erro padr√£o para o estimador m√©dia amostral apresenta f√≥rmula conhecida, mas h√° casos em que n√£o dispomos de f√≥rmulas. Suponha que haja interesse em comparar os dois grupos de ratos em rela√ß√£o aos tempos medianos. Temos: md(X)=94 e md(Y)=46. A diferen√ßa √© 48, maior que a diferen√ßa para as m√©dias. Com base na mediana, o tratamento prolonga a vida?\n\n\n6.1.3 Estimativa bootstrap do erro padr√£o\nConsidera√ß√µes:\n\n\\(\\underset{\\sim}{x} = (x_1,\\,x_2\\,\\ldots,\\,x_n)\\): vetor de dados observado (amostra original de tamanho \\(n\\))\n\\(s(\\underset{\\sim}{x})\\): estat√≠stica de interesse (por exemplo, m√©dia amostral)\nUma amostra bootstrap \\(\\underset{\\sim}{x}^*=(x_1^*, x_2^*, \\ldots, x_n^*)\\) √© obtida pela amostragem aleat√≥ria de tamanho \\(n\\), com reposi√ß√£o, de \\(\\underset{\\sim}{x}\\). Por exemplo, com \\(n=7\\), poder√≠amos obter \\(\\underset{\\sim}{x}^*=(x_1^*, x_2^*, \\ldots, x_n^*)=(x_5, x_7, x_5, x_4, x_7, x_3, x_1)\\).\n\nO algoritmo bootstrap\n\ngerar \\(B\\) amostras bootstrap independentes: \\(\\underset{\\sim}{x}^{*^{1}}, \\underset{\\sim}{x}^{*^{2}}, \\ldots, \\underset{\\sim}{x}^{*^{B}},\\) cada uma de tamanho \\(n\\) e \\(50 \\leq B \\leq 200\\).\ncalcular \\(s(\\underset{\\sim}{x}^{*^{b}})\\), \\(b=1, 2, \\ldots, B\\). \\(s(\\underset{\\sim}{x}^{*^{b}})\\) √© denominada r√©plica bootstrap de \\(s(\\underset{\\sim}{x})\\)\ncalcular \\(\\hat{ep}_{boot}=\\hat{ep}_{B}=\\sqrt{ \\dfrac{\\sum_{b=1}^{B}[s(\\underset{\\sim}{x}^{*^{b}})-s(.)]^2}{B-1}},\\)em que \\(s(.)=\\frac{\\sum_{b=1}^{B}s(\\underset{\\sim}{x}^{*^{b}})}{B}\\)\n\nSintaxe do R para calcular a estimativa bootstrap do erro padr√£o da m√©dia do tempo de sobrevida dos ratos do grupo tratamento.\n\nset.seed(1234)\n# grupo tratamento (amostra original)\nx &lt;- c(94, 197, 16, 38, 99, 141, 23) \nn &lt;- length(x)\ns &lt;- 0  # estat√≠stica de interesse\nB &lt;- 50 # no. de amostras bootstrap \n\nfor(i in 1:B){\n   # r√©plica bootstrap para o estimador m√©dia\n   s[i] &lt;- mean(sample(x,n,replace=TRUE)) \n }\n# estimativa bootstrap para o erro padr√£o da m√©dia\nep &lt;-  sd(s); ep\n\n[1] 27.41873\n\n\nEstimativas bootstrap do erro padr√£o da m√©dia e da mediana do tempo de sobreviv√™ncia dos ratos do grupo tratamento. A mediana √© menos acurada (erros padr√µes maiores) que a m√©dia para esse conjunto de dados.\n\n\n\n\\(B\\)\n50\n100\n250\n500\n1000\n\\(\\infty\\)\n\n\n\n\nm√©dia\n19.72\n23.63\n22.32\n23.79\n23.02\n23.36\n\n\nmediana\n32.21\n36.35\n34.46\n36.72\n36.48\n37.83\n\n\n\nFormaliza√ß√£o:\n\n\\(\\underset{\\sim}{X} = X_1, X_2,\\dots, X_n\\): amostra aleat√≥ria de uma f.d.a. \\(F\\)\n\\(\\underset{\\sim}{x} = (x_1, x_2,\\ldots, x_n)\\): amostra aleat√≥ria observada de \\(F\\)\n\\(\\Theta=t(F)\\): par√¢metro (\\(\\Theta\\) uma fun√ß√£o de \\(F\\))\n\\(\\hat{\\Theta}=s(\\underset{\\sim}{X})\\): estimador para \\(\\Theta\\)\n\\(\\hat{\\theta}=s(\\underset{\\sim}{x})\\): estimativa para \\(\\Theta\\)\nQu√£o acurado √© o estimador \\(\\hat{\\Theta}\\)?\nSeja \\(\\hat{F}\\) a distribui√ß√£o emp√≠rica que atribui a probabilidade \\(1/n\\) para cada valor observado \\(x_i\\), \\(i=1,2,\\ldots, n\\). A amostra bootstrap \\(\\underset{\\sim}{x}^*\\) √© definida como a amostra aleat√≥ria com reposi√ß√£o de tamanho \\(n\\) extra√≠da de \\(\\hat{F}\\). \\[\\underset{\\sim}{x}^*=(x_1^*,\\,x_2^*\\,\\ldots,\\,x_n^*)\\] \\[\\hat{F}\\longrightarrow (x_1^*,\\,x_2^*\\,\\ldots,\\,x_n^*)\\] Nota:\n\\(\\underset{\\sim}{x}^*\\): o s√≠mbolo \\(*\\) indica que a amostra n√£o √© a original \\(\\underset{\\sim}{x}\\), mas uma vers√£o aleatorizada(reamostrada) de \\(\\underset{\\sim}{x}\\)\na cada amostra bootstrap corresponde uma r√©plica bootstrap de \\(\\hat\\theta\\), \\(\\hat\\theta^* =s(\\underset{\\sim}{x}^*)\\)\n\\(ep_F(\\hat\\Theta)\\) √© estimado por \\(ep_{\\hat{F}}(\\hat\\Theta^*)\\), chamado para o erro padr√£o \\(\\hat\\Theta\\)\nRaramente faz-se necess√°rio \\(B\\geq 200\\) para estimar erro padr√£o; valores (muito) maiores s√£o necess√°rios, por exemplo, para IC bootstrap.\n\\(\\displaystyle\\lim_{B \\to \\infty} \\hat{ep}_B=ep_{\\hat{F}}=ep_{\\hat{F}}(\\hat\\Theta^*)\\)\nO estimador bootstrap ideal \\(ep_{\\hat{F}}(\\hat\\Theta^*)\\) e sua aproxima√ß√£o \\(\\hat{ep}_B\\) s√£o chamados estimadores bootstrap n√£o-param√©tricos, pois baseiam-se em \\(\\hat{F}\\), o estimador n√£o-param√©trico de \\(F\\).\ntotal de amostras bootstrap distintas (combina√ß√£o com repeti√ß√£o): \\[\\binom{2n -1}{n}.\\]\nNo R: ver as fun√ß√µes factorial(), choose(), combn(), combinations().\n\n\n6.1.3.1 Exemplo\nDados de faculdades americanas de direito. Popula√ß√£o: \\(N=82\\) faculdades. Amostra aleat√≥ria: \\(n=15\\) faculdades. Vari√°veis analisadas: LSAT (escore m√©dio em um teste), GPA (pontua√ß√£o m√©dia na faculdade).\n\n\n\nescola\nLSAT\nGPA\nescola\nLSAT\nGPA\n\n\n\n\n1\n576\n3,39\n9\n651\n3,36\n\n\n2\n635\n3,30\n10\n605\n3,13\n\n\n3\n558\n2,81\n11\n653\n3,12\n\n\n4\n578\n3,03\n12\n575\n2,74\n\n\n5\n666\n3,44\n13\n545\n2,76\n\n\n6\n580\n3,07\n14\n572\n2,88\n\n\n7\n555\n3,00\n15\n594\n2,96\n\n\n8\n661\n3,43\n\n\n\n\n\n\n\nFa√ßamos \\(Y\\)=LSAT e \\(Z\\)=GPA. A estat√≠stica (estimador) de interesse √© o coeficiente de correla√ß√£o amostral entre as vari√°veis \\(Y\\) e \\(Z\\): \\[\\hat{\\Theta}=corr(Y,Z)=\\dfrac{Cov(Y,Z)}{DP(Y).DP(Z)}=\\dfrac{\\sum_{i=1}^n(Y_i-\\bar{Y})(Z_i-\\bar{Z})/n}{DP(Y).DP(Z)}\\]\nPara os dados observados, a estimativa do coeficiente de correla√ß√£o amostral √© 0.776. Qu√£o acurado √© o estimador?\n\n\nEstimativas bootstrap do erro padr√£o para \\(\\hat{\\Theta} = corr(Y,Z)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\)\n25\n50\n100\n200\n400\n800\n1600\n3200\n\n\n\n\n\\(\\hat{ep}_B\\)\n0,140\n0,142\n0,151\n0,143\n0,141\n0,137\n0,133\n0,132\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nNo caso de valores extremos inflacionarem fortemente \\(\\hat{ep}_B\\), uma medida mais robusta para o estimador bootstrap do erro padr√£o √© desej√°vel. (ver Efron \\(\\&\\) Tibshirani (1993)).\n\n\n\n\n\n\n\n\nNota\n\n\n\nInfer√™ncias baseadas na distribui√ß√£o normal s√£o ‚Äúquestion√°veis‚Äù quando o histograma das r√©plicas bootstrap indica forte assimetria.\n\n\n\n\n6.1.3.2 Exerc√≠cios:\n\nConsidere \\(B = 3200\\). Para cada uma das 3200 amostras bootstrap, obtenha a r√©plica bootstrap \\(\\hat{\\theta}^*=corr(y^*,z^*)\\). Fa√ßa o histograma das r√©plicas.\nA Tabela 3.2, p.¬†21 do livro texto, apresenta os dados populacionais das 82 faculdades. Selecione 3200 amostras aleat√≥rias de tamanho \\(n = 15\\). Para cada uma dessas amostra calcule o coeficiente de correla√ß√£o e fa√ßa o histograma.\n\n\n\n\n6.1.4 Bootstrap Param√©trico\nO estimador bootstrap param√©trico do erro padr√£o √© definido por \\[ep_{\\hat{F}_{par}}(\\hat\\Theta^*),\\] em que \\(\\hat{F}_{par}\\) √© um estimador de \\(F\\) derivado do modelo param√©trico para os dados.\nPara os dados das faculdades: Vamos supor que a popula√ß√£o (LSAT, GPA) possa ser descrita por um modelo param√©trico normal bivariado \\(F\\). Estimamos \\(F\\) por \\(\\hat{F}_{normal}\\), que denota a f.d.a. de uma normal bivariada com vetor de m√©dias e matriz de covari√¢ncias \\((\\bar{y},\\bar{z})\\) e \\(\\dfrac{1}{14}\\left(\n\\begin{array}{ll}\n\\sum(y_i-\\bar{y})^2            &  \\sum(y_i-\\bar{y})(z_i-\\bar{z}) \\\\\n\\sum(y_i-\\bar{y})(z_i-\\bar{z}) &  \\sum(z_i-\\bar{z})^2  \\\\\n\\end{array}\n\\right)\\).\nO estimador bootstrap param√©trico do erro padr√£o da correla√ß√£o \\(\\hat{\\Theta}\\) ser√° dado por \\(ep_{\\hat{F}_{normal}}(\\hat\\Theta^*)\\). Esse estimador bootstrap ideal ser√° aproximado por \\(\\hat{ep}_{B}\\) (conforme algoritmo a seguir).\nO algoritmo bootstrap:\n\nextrair \\(B\\) amostras de tamanho \\(n\\) de \\(\\hat{F}_{par}\\): \\(\\underset{\\sim}{x}^{*^{1}}, \\underset{\\sim}{x}^{*^{2}}, \\ldots, \\underset{\\sim}{x}^{*^{B}}\\)\ncalcular \\(s(\\underset{\\sim}{x}^{*^{b}})\\), \\(b=1, 2, \\ldots, B\\). \\(s(\\underset{\\sim}{x}^{*^{b}})\\) √© a r√©plica bootstrap de \\(s(\\underset{\\sim}{x})\\)\ncalcular \\(\\hat{ep}_{boot}=\\hat{ep}_{B}=\\sqrt{ \\dfrac{\\sum_{b=1}^{B}[s(\\underset{\\sim}{x}^{*^{b}})-s(.)]^2}{B-1}},\\)em que \\(s(.)=\\frac{\\sum_{b=1}^{B}s(\\underset{\\sim}{x}^{*^{b}})}{B}\\)\n\nNo exemplo das faculdades, assumindo o modelo normal bivariado, extra√≠mos \\(B\\) amostras de tamanho \\(n=15\\) de \\(\\hat{F}_{normal}\\), calculamos o coeficiente de correla√ß√£o para cada amostra e, por fim, calculamos o desvio padr√£o desses coeficientes de correla√ß√£o. Usando \\(B=3200\\) encontramos \\(\\hat{ep}_{B}=0.124\\), que √© pr√≥ximo ao valor 0.131 obtido com o bootstrap n√£o-param√©trico.\nA f√≥rmula te√≥rica para o erro padr√£o do coeficiente de correla√ß√£o √© \\(\\dfrac{1-\\hat{\\Theta}^2}{\\sqrt{n-3}}\\), com \\(\\hat{\\Theta}=corr(Y,Z)\\). Vimos que \\(\\hat{\\theta}=0.776\\), o que resulta a estimativa 0.115 para o erro padr√£o do coeficiente de correla√ß√£o entre as vari√°veis \\(Y\\)=LSAT e \\(Z\\)=GPA.\nTransforma√ß√£o de Fisher para o coeficiente de correla√ß√£o \\(\\hat{\\Theta}\\): \\(\\hat{\\zeta}=0.5\\log\\left(\\dfrac{1+\\hat{\\Theta}}{1-\\hat{\\Theta}}\\right)\\). Assim, \\(\\hat{\\zeta}\\) tem distribui√ß√£o aproximadamente normal com m√©dia \\(0.5\\log\\left(\\dfrac{1+\\Theta}{1-\\Theta}\\right)\\) e vari√¢ncia \\(\\dfrac{1}{n-3}\\). O erro padr√£o para \\(\\hat{\\zeta}\\) √© \\(\\sqrt{\\dfrac{1}{n-3}}\\). Para o exemplo das faculdades, o valor √© \\(\\dfrac{1}{\\sqrt{12}}=0.289\\).\nA t√≠tulo de compara√ß√£o com o procedimento bootstrap, a estat√≠stica (estimador) \\(\\hat{\\zeta}\\) foi estimado em cada uma das \\(B=3200\\) amostras bootstrap. O desvio padr√£o das r√©plicas bootstrap resultou 0.290 (muito pr√≥ximo ao valor te√≥rico 0.289). Histogramas para as correla√ß√µes \\(\\hat{\\theta}^*\\)e para os \\(\\hat{\\zeta}^*\\).\n\n\n\n\n\n\nImportante\n\n\n\nMuitas f√≥rmulas para os erros padr√µes s√£o aproxima√ß√µes baseadas na teoria normal e isso ‚Äúexplica‚Äù os resultados pr√≥ximos obtidos com o uso do bootstrap param√©trico que extrai amostras a partir da distribui√ß√£o normal.\n\n\nVantagens do bootstrap sobre os m√©todos tradicionais:\n\nbootstrap n√£o-param√©trico: n√£o √© necess√°rio fazer suposi√ß√µes de modelos param√©tricos para a popula√ß√£o;\nbootstrap param√©trico: possibilita estimar erros padr√µes em problemas para os quais n√£o h√° f√≥rmulas para os erros padr√µes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>M√©todos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#jackknife",
    "href": "boot.html#jackknife",
    "title": "6¬† M√©todos de Reamostragem",
    "section": "6.2 Jackknife",
    "text": "6.2 Jackknife\n\n6.2.1 Introdu√ß√£o\nJackknife √© uma t√©cnica para estimar vi√©s e erro padr√£o de estimadores. √â uma t√©cnica que antecede o bootstrap e foi proposta no trabalho pioneiro Quenouille (1949) para reduzir vi√©s do estimador da correla√ß√£o serial.\nFormaliza√ß√£o:\n\n\\(\\underset{\\sim}{X} = (X_1, X_2, \\dots, X_n)\\): amostra aleat√≥ria de uma f.d.a. \\(F\\)\n\\(\\underset{\\sim}{x} = (x_1, x_2, \\dots, x_n)\\): amostra aleat√≥ria observada de \\(F\\)\n\\(\\Theta=t(F)\\): par√¢metro (\\(\\Theta\\) uma fun√ß√£o de \\(F\\))\n\\(\\hat{\\Theta}=s(\\underset{\\sim}{X})\\): estimador para \\(\\Theta\\)\n\\(\\hat{\\theta}=s(\\underset{\\sim}{x})\\): estimativa para \\(\\Theta\\)\n\nO jackknife toma como base \\(n\\) amostras de tamanho \\(n-1\\) selecionadas da amostra aleat√≥ria observada. A \\(i\\)-√©sima amostra jackknife consiste da amostra observada com a \\(i\\)-√©sima observa√ß√£o removida, \\(i=1,2, \\ldots, n\\): \\[{\\underset{\\sim}{x}}_{(i)}=(x_1,x_2,\\ldots, x_{i-1},x_{i+1}, \\ldots, x_n).\\]\n\n\n6.2.2 Estimador do vi√©s\nPara cada amostra \\(i\\) jackknife, √© obtida a r√©plica jackknife \\(\\hat{\\theta}_{(i)}=s({\\underset{\\sim}{x}}_{(i)})\\) e o estimador jackknife do vi√©s de \\(\\hat{\\Theta}\\) √© dado por: \\[\\widehat{\\mbox{vi√©s}}_{jack} =(n-1)( \\hat{\\Theta}_{(\\cdot)} - \\hat{\\Theta} ),\\] em que \\(\\hat{\\Theta}_{(\\cdot)}=\\sum_{i=1}^n\\dfrac{\\hat{\\Theta}_{(i)}}{n}\\).\n\n\n6.2.3 Estimado do erro padr√£o\nO estimador jackknife do erro padr√£o de \\(\\hat{\\Theta}\\) pode ser escrito da seguinte maneira: \\[\\widehat{ep}_{jack} =\\left[ \\dfrac{n-1}{n}\\sum_{i=1}^n (\\hat{\\Theta}_{(i)} - \\hat{\\Theta}_{(\\cdot)})^2\\right]^{\\frac{1}{2}},\\] em que \\(\\hat{\\Theta}_{(\\cdot)}=\\sum_{i=1}^n\\dfrac{\\hat{\\Theta}_{(i)}}{n}\\).\nAlgumas considera√ß√µes importantes:\n\nBootstrap: amostragem aleat√≥ria com reposi√ß√£o;\nJackknife: amostras fixas;\nJackknife requer o c√°lculo do estimador apenas para \\(n\\) amostras;\nA acur√°cia do estimador jackknife do erro padr√£o depende de qu√£o pr√≥ximo o estimador √© da linearidade. Para fun√ß√µes fortemente n√£o lineares, o jackknife pode ser ineficiente;\nEstimador linear: \\(\\hat{\\Theta}=s(\\underset{\\sim}{X})=\\mu +\\frac{1}{n}\\sum_{i=1}^n\\alpha(X_i).\\)\n\n\n6.2.3.1 Exemplo\nConsidere a amostra observada: \\(\\underset{\\sim}{x}=(10,26,30,40,48)\\) e o estimador: mediana(\\(\\hat{\\Theta}\\)). As amostras e r√©plicas jackknife s√£o dadas, respectivamente, por:\n\n\\({\\underset{\\sim}{x}}_{(1)}=(26,30,40,48)\\) e \\(\\hat{\\theta}_{(1)}=35;\\)\n\\({\\underset{\\sim}{x}}_{(2)}=(10,30,40,48)\\) e \\(\\hat{\\theta}_{(2)}=35;\\)\n\\({\\underset{\\sim}{x}}_{(3)}=(10,26,40,48)\\) e \\(\\hat{\\theta}_{(3)}=33;\\)\n\\({\\underset{\\sim}{x}}_{(4)}=(10,26,30,48)\\) e \\(\\hat{\\theta}_{(4)}=28;\\)\n\\({\\underset{\\sim}{x}}_{(5)}=(10,26,30,40)\\) e \\(\\hat{\\theta}_{(5)}=28.\\)\n\nDesta forma, a estimativa jackknife do erro padr√£o de \\(\\hat{\\Theta}\\): \\[\\widehat{ep}_{jack} =\\left[ \\dfrac{4}{5}\\sum_{i=1}^5 (\\hat{\\theta}_{(i)} - \\hat{\\theta}_{(\\cdot)})^2\\right]^{\\frac{1}{2}}\\approx 6.38,\\] com \\(\\hat{\\theta}_{(\\cdot)}=\\sum_{i=1}^5\\dfrac{\\hat{\\theta}_{(i)}}{5}=31.8.\\)\nA seguir, a sintaxe do R para calcular a estimativa jackknife do erro padr√£o para a mediana.\n\nx &lt;- c(10,26,30,40,48)\nn &lt;- length(x)\n\nest.jack &lt;- 0\n\nfor(i in 1:n){ \n  est.jack[i] &lt;- median(x[-i])\n}\n\nep.jack &lt;- sqrt(((n-1)^2/n)*var(est.jack))\nep.jack\n\n[1] 6.374951",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>M√©todos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#intervalos-de-confian√ßa",
    "href": "boot.html#intervalos-de-confian√ßa",
    "title": "6¬† M√©todos de Reamostragem",
    "section": "6.3 Intervalos de Confian√ßa",
    "text": "6.3 Intervalos de Confian√ßa\n\n\n\n\n\n\nNota√ß√£o\n\n\n\n\n\\(\\underset{\\sim}{X} = (X_1, x_2, \\dots, X_n)\\): amostra aleat√≥ria de uma f.d.a. \\(F\\)\n\\(\\underset{\\sim}{x} = (x_1, x_2, \\dots, x_n)\\): amostra aleat√≥ria observada de \\(F\\)\n\\(\\Theta=t(F)\\): par√¢metro (\\(\\Theta\\) uma fun√ß√£o de \\(F\\))\n\\(\\hat{\\Theta}=s(\\underset{\\sim}{X})\\): estimador para \\(\\Theta\\)\n\\(\\hat{\\theta}=s(\\underset{\\sim}{x})\\): estimativa para \\(\\Theta\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>M√©todos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#intervalos-de-confian√ßa-normal-padr√£o",
    "href": "boot.html#intervalos-de-confian√ßa-normal-padr√£o",
    "title": "6¬† M√©todos de Reamostragem",
    "section": "6.4 Intervalos de Confian√ßa Normal Padr√£o",
    "text": "6.4 Intervalos de Confian√ßa Normal Padr√£o\nSuponha que \\(\\hat{\\Theta}\\sim N(\\Theta, ep(\\hat{\\Theta})^2)\\). Portanto, se \\(ep(\\hat{\\Theta})\\) √© conhecido temos que:\n\\[Z=\\dfrac{\\hat{\\Theta} - \\Theta}{ep(\\hat\\Theta)} \\sim N(0,1),\\] e \\[IC_z\\left(100(1-\\alpha)\\%,\\Theta\\right)=\\hat{\\Theta} \\pm z_{\\frac{\\alpha}{2}}ep(\\hat{\\Theta}).\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>M√©todos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#intervalo-de-confian√ßa-t-student",
    "href": "boot.html#intervalo-de-confian√ßa-t-student",
    "title": "6¬† M√©todos de Reamostragem",
    "section": "6.5 Intervalo de Confian√ßa t-Student",
    "text": "6.5 Intervalo de Confian√ßa t-Student\nSuponha que \\(\\hat{\\Theta}\\sim N(\\Theta, ep(\\hat{\\Theta})^2)\\) e se \\(ep(\\hat{\\Theta})\\) √© desconhecido, portanto\n\\[Z=\\dfrac{\\hat{\\Theta} - \\Theta}{\\hat{ep}(\\hat\\Theta)} \\sim \\mbox{t-Student}(n-1),\\]\ne\n\\[IC_t\\left(100(1-\\alpha)\\%,\\Theta\\right)=\\hat{\\Theta} \\pm t_{(n-1,\\frac{\\alpha}{2})}\\hat{ep}(\\hat{\\Theta}).\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>M√©todos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#ic-bootstrap-t",
    "href": "boot.html#ic-bootstrap-t",
    "title": "6¬† M√©todos de Reamostragem",
    "section": "6.6 IC bootstrap-\\(t\\)",
    "text": "6.6 IC bootstrap-\\(t\\)\nAgora vamos definir\n\\[Z^{*^b}=\\dfrac{ \\hat{\\Theta}^{*^b} - \\hat{\\Theta} }{\\hat{ep}^{*^b}},\\] em que \\(\\hat{\\Theta}^{*^b}=s(\\underset{\\sim}{X}^{*^b})\\) e \\(\\hat{ep}^{*^b}\\) s√£o obtidos para cada amostra bootstrap \\(\\underset{\\sim}{x}^*\\). O percentil \\(100 \\cdot \\alpha\\) de \\(Z^{*^b}\\) √© estimado pelo valor \\(\\hat{t}^{(\\alpha)}\\), tal que \\[ \\#\\{Z^{*^b}\\leq \\hat{t}^{(\\alpha)} \\}/B=\\alpha.\\]\nPor exemplo, se \\(B=100\\) e a confian√ßa para o intervalo √© de 90\\(\\%\\), a estimativa \\(\\hat{t}^{(0.05)}\\) ser√° o quinto maior valor de \\(Z^{*^b}\\) e a estimativa \\(\\hat{t}^{(0.95)}\\) ser√° o nonag√©simo quinto maior valor de \\(Z^{*^b}\\).\nEste procedimento estima a distribui√ß√£o de \\(Z\\) (quantidade pivotal) diretamente dos dados. N√£o √© necess√°rio a suposi√ß√£o te√≥rica de normalidade e sua distribui√ß√£o √© a mesma para qualquer . O intervalo (bilateral) bootstrap-t de confian√ßa \\(100\\times(1-2\\alpha)\\%\\) para o par√¢metro \\(\\Theta\\), denotado por \\(IC_t^*\\left(100(1-2\\alpha)\\%,\\Theta\\right)\\), √© definido por:\n\\[ \\left[\\hat{\\Theta} - \\hat{t}^{(1-\\alpha)}\\hat{ep}(\\hat{\\Theta}), \\hat{\\Theta} - \\hat{t}^{(\\alpha)}\\hat{ep}(\\hat{\\Theta})\\right].\\] Emprega-se estimativas para \\(\\hat{\\Theta}\\) e \\(\\hat{ep}(\\hat{\\Theta})\\); n√£o sendo poss√≠vel, o erro padr√£o √© estimado usando bootstrap ou jackknife.\n\n\n\n\n\n\nImportante\n\n\n\nSe \\(B\\cdot\\alpha\\) n√£o resultar um n√∫mero inteiro?\n\nEfron \\(\\&\\) Tibshirani (1993): supondo \\(\\alpha \\leq 0.5\\), fa√ßa \\(k=\\mbox{Int}[(B+1)\\cdot\\alpha]\\) (o maior inteiro \\(\\leq (B+1)\\cdot\\alpha\\) ) e defina os percentis emp√≠ricos de ordem \\(100\\cdot\\alpha\\) e \\(100\\cdot(1-\\alpha)\\), respectivamente, pelo \\(k\\)-√©simo e \\((B+1-k)\\)-√©simo maiores valores de \\(Z^{*^b}\\).\nDavidson \\(\\&\\) Hinkley (1997): interpola√ß√£o dos percentis.\n\n\n\n\n\n\n\n\n\nConsidera√ß√µes\n\n\n\n\nEste intervalo pode ser fortemente influenciado por poucos valores discrepantes.\nEm geral, √© necess√°rio um valor de \\(B\\) muito superior a 200.\nPara grandes amostras, o IC bootstrap-t pode estar mais pr√≥ximo do n√≠vel de cobertura desejado do que os IC Normal ou t-Student.\nAbaixo, percentis da distribui√ß√£o \\(t\\)-Student com 8 gl, normal padr√£o e distribui√ß√£o bootstrap de \\(Z^{*^b}\\) (para o grupo controle no experimento com os ratos; \\(B=1000\\)).\n\n\n\n\nCompara√ß√£o dos percentis de diferentes distribui√ß√µes.\n\n\npercentil\n5\n10\n90\n95\n\n\n\n\n\\(t_8\\)\n-1.86\n-1.40\n1.40\n1.86\n\n\nNormal\n-1.65\n-1.28\n1.28\n1.65\n\n\nbootstrap-t\n-4.53\n-2.01\n1.19\n1.53\n\n\n\nO IC bootstrap-\\(t\\) para \\(\\Theta\\) (m√©dia do tempo de sobreviv√™ncia de ratos n√£o submetidos ao tratamento (grupo controle)):\n\\[\\left[ 56.22-1.53 \\times 13.33, \\ 56.22+4.53  \\times 13.33\\right]=\\left[35.82, \\ 116.74\\right],\\] usado a estimativa para o erro padr√£o da m√©dia.\n\n\n\n\n\n\nConsidera√ß√µes Finais\n\n\n\n\nos valores dos percentis bootstrap-\\(t\\) podem n√£o ser sim√©tricos em rela√ß√£o ao zero.\naplic√°vel para estat√≠sticas de localiza√ß√£o (m√©dia, mediana etc).\nCautela no uso envolvendo pequenas amostras (limites do intervalo fora do espa√ßo param√©trico).\no \\(IC^*_t\\) n√£o √© transformation-respecting (ver pr√≥xima se√ß√£o)\n\n\n\n\n6.6.1 Intervalos de Confian√ßa bootstrap percentil\nO intervalo bootstrap percentil de confian√ßa \\(100(1-2\\alpha)\\%\\), denotado por \\(IC_p^*\\left(100(1-2\\alpha)\\%,\\Theta \\right)\\), √© definido por\n\\[\\left[ \\hat\\Theta^{*(\\alpha)}, \\ \\hat\\Theta^{*(1-\\alpha)}\\right],\\] em que \\(\\hat\\Theta^{*(\\alpha)}\\) √© o percentil de ordem 100\\(\\alpha\\) da distribui√ß√£o bootstrap de \\(\\hat\\Theta^*\\). Esta defini√ß√£o refere-se a um n√∫mero infinito de r√©plicas.\nEm situa√ß√µes pr√°ticas, o intervalo √© aproximado com base nos resultados de \\(B\\) r√©plicas bootstrap:\n\\[\\left[ \\hat\\Theta_B^{*(\\alpha)}, \\ \\hat\\Theta_B^{*(1-\\alpha)}\\right],\\] em que \\(\\hat\\Theta_B^{*(\\alpha)}\\) √© dado pelo percentil de ordem 100\\(\\alpha\\) dos valores de \\(\\hat\\theta_B^{*(b)}\\), isto √©, o \\((B\\cdot\\alpha)\\)-√©simo valor ordenado da lista das \\(B\\) r√©plicas de \\(\\hat\\theta_B^{*(b)}\\).\n\n\n\n\n\n\nNota\n\n\n\nSe a distribui√ß√£o de \\(\\hat\\Theta^{*}\\) √© aproximadamente normal, os intervalos percentil e normal padr√£o s√£o pr√≥ximos.\n\n\n\n6.6.1.1 Exemplo\nPara uma amostra: , \\(n=10\\), de uma distribui√ß√£o (popula√ß√£o) normal padr√£o, considere o par√¢metro de interesse \\(\\Theta\\). \\(\\Theta=\\exp(\\mu)\\), sendo \\(\\mu=0\\). Sendo a estimativa para \\(\\Theta\\): \\(\\hat{\\theta}=\\exp(\\bar{x})\\). Al√©m disso, use o bootstrap n√£o param√©trico: \\(B=1000\\) r√©plicas \\(\\hat{\\theta}^*\\) usando a amostra observada: (1.669, -0.411, -0.322, 0.746, -0.868, -0.874, 1.011, -0.173, 0.021, 1.482). Resultando na estimativa para \\(\\Theta\\): \\(\\hat{\\theta}=\\exp(\\bar{x})=1.256\\).\n\nPercentis de \\(\\hat\\theta^*\\) com base em 1000 r√©plicas bootstrap.\n\n\n2.5%\n5%\n10%\n16%\n50%\n84%\n90%\n95%\n97.5%\n\n\n\n\n0.74\n0.81\n0.89\n0.95\n1.25\n1.65\n1.77\n2.00\n2.13\n\n\n\nDesta maneira, temos que\n\\[IC_p^*\\left(95\\%,\\Theta\\right)=\\left[0.74, 2.13\\right],\\]\ne\n\\[IC_p^*\\left(90\\%,\\Theta\\right)=\\left[0.81, 2.00\\right].\\]\nAbaixo um c√≥digo R para o exemplo.\n\nset.seed(1234)\nn &lt;- 10\nx &lt;- rnorm(n) # amostra observada\nB &lt;- 1000     # no. de r√©plicas bootstrap\ntheta &lt;- 0\nalfa &lt;- c(0.025,0.05,0.10,0.16,0.50,0.84,0.90,0.95,0.975)\n\nfor(i in 1:B){\n  a &lt;- sample(x,n,replace=TRUE)  # amostra bootstrap\n  theta[i] &lt;- exp(mean(a))       # r√©plica bootstrap \n}\n\nexp(mean(x)) # estimativa\n\n[1] 0.6817056\n\nsz &lt;- sort(theta)\nrbind(100*alfa, round(sz[c(B*alfa)],2)) \n\n     [,1] [,2]  [,3]  [,4]  [,5]  [,6] [,7] [,8] [,9]\n[1,] 2.50 5.00 10.00 16.00 50.00 84.00   90 95.0 97.5\n[2,] 0.37 0.42  0.48  0.52  0.71  0.93    1  1.1  1.2\n\n\nAgora iremos considerar outra abordagem. Seja \\(\\Phi=\\log(\\Theta)\\) e \\(\\hat\\Phi=\\log(\\hat\\Theta)=\\bar{X}\\). Usando \\(IC_z\\left(95\\%,\\Phi \\right)= \\hat{\\Phi} \\pm z_{\\frac{\\alpha}{2}}\\hat{ep}(\\hat{\\Phi})\\), resulta \\(\\left[ 0.228 \\pm 1.96 \\times 0.28 \\right]=\\left[-0.32, 0.78\\right]\\), em que \\(\\hat{ep}(\\hat{\\Phi})\\) √© o estimador para o erro padr√£o de \\(\\hat\\Phi\\). Fazendo a transforma√ß√£o inversa, o intervalo para \\(\\Theta\\) √© \\(\\left[0.73, 2.18\\right]\\). Note que o intervalo percentil para \\(\\Theta\\) assemelha-se ao intervalo normal padr√£o constru√≠do para uma transforma√ß√£o apropriada de \\(\\Theta\\) e transformado para a escala de \\(\\Theta\\).\n\n\n\n\n\n\nImportante\n\n\n\nVantagem do m√©todo percentil: N√£o √© necess√°rio conhecer a transforma√ß√£o (normalizadora). O m√©todo percentil incorpora automaticamente a transforma√ß√£o adequada.\n\n\n\n\n\n\n\n\nLema\n\n\n\nSuponha a transforma√ß√£o \\(\\hat\\Phi=m(\\hat\\Theta)\\) e, consequentemente, \\(\\hat\\Phi\\sim N(\\Phi,c^2)\\), para algum erro padr√£o \\(c\\). Ent√£o, o intervalo percentil para \\(\\Theta\\) ser√° dado por \\[\\left[ m^{-1}(\\hat\\Phi -z_{\\frac{\\alpha}{2}}.c), m^{-1}(\\hat\\Phi +z_{\\frac{\\alpha}{2}}.c)\\right].\\]\n\n\n\n\n\n\n\n\nNota\n\n\n\n\nEm situa√ß√µes nas quais o uso de IC padr√£o √© adequado\\(^*\\), o IC percentil produz resultados pr√≥ximos ao do IC padr√£o.\nEm situa√ß√µes nas quais o uso do IC padr√£o √© adequado apenas ap√≥s uma transforma√ß√£o no par√¢metro, a aplica√ßao do m√©todo percentil automaticamente contempla essa transforma√ß√£o.\n\n\n\n\n\n\n\n\n\nConsidera√ß√µes Finais\n\n\n\n\nPropriedade : o intervalo percentil para qualquer transforma√ß√£o (monot√¥nica) \\(\\Phi=m(\\Theta)\\) do par√¢metro \\(\\Theta\\) √© dado por \\[\\left[ m(\\hat\\Theta^{*(\\alpha)}), \\ m(\\hat\\Theta^{*(1-\\alpha)}\\right].\\]\nA propriedade tamb√©m √© v√°lida para o intervalo aproximado com base nos resultados de \\(B\\) r√©plicas bootstrap: \\[\\left[ m(\\hat\\Theta_B^{*(\\alpha)}), \\ m(\\hat\\Theta_B^{*(1-\\alpha)})\\right].\\]\nPropriedade range-preserving: produz intervalos com limites dentro do espa√ßo param√©trico.\n\n\n\n\n\n\n6.6.2 Intervalos de Confian√ßa bootstrap - vers√µes aprimoradas\nO intervalo bootstrap-\\(t\\) apresenta boa probabilidade de cobertura te√≥rica, mas tende a ser irregular na pr√°tica. J√° o intervalo bootstrap percentil √© menos irregular, mas apresenta probabilidade de cobertura menos satisfat√≥ria.\nProposta: intervalo bootstrap \\(BC_a\\) (bias-corrected and accelerated) e \\(ABC\\) (approximate bootstrap confidence).\nsubstancial melhoria na pr√°tica e na teoria. corre√ß√£o de vi√©s do estimador. o \\(ABC\\) exige menos esfor√ßo computacional do que o \\(BC_a\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>M√©todos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "mc.html",
    "href": "mc.html",
    "title": "7¬† M√©todos de Monte Carlo",
    "section": "",
    "text": "7.1 Introdu√ß√£o",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>M√©todos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#integra√ß√£o-de-monte-carlo",
    "href": "mc.html#integra√ß√£o-de-monte-carlo",
    "title": "7¬† M√©todos de Monte Carlo",
    "section": "7.2 Integra√ß√£o de Monte Carlo",
    "text": "7.2 Integra√ß√£o de Monte Carlo",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>M√©todos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#erro-de-monte-carlo",
    "href": "mc.html#erro-de-monte-carlo",
    "title": "7¬† M√©todos de Monte Carlo",
    "section": "7.3 Erro de Monte Carlo",
    "text": "7.3 Erro de Monte Carlo",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>M√©todos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#monte-carlo-via-fun√ß√£o-de-import√¢ncia",
    "href": "mc.html#monte-carlo-via-fun√ß√£o-de-import√¢ncia",
    "title": "7¬† M√©todos de Monte Carlo",
    "section": "7.4 Monte Carlo via Fun√ß√£o de Import√¢ncia",
    "text": "7.4 Monte Carlo via Fun√ß√£o de Import√¢ncia",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>M√©todos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#m√©todo-de-m√°xima-verossimilhan√ßa",
    "href": "mc.html#m√©todo-de-m√°xima-verossimilhan√ßa",
    "title": "7¬† M√©todos de Monte Carlo",
    "section": "7.5 M√©todo de M√°xima Verossimilhan√ßa",
    "text": "7.5 M√©todo de M√°xima Verossimilhan√ßa",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>M√©todos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Falk, Ruma. 2014. ‚ÄúA Closer Look at the Notorious Birthday\nCoincidences.‚Äù Teaching Statistics 36 (2): 41‚Äì46. https://doi.org/10.1111/test.12014.\n\n\nHodgson, Ted, and Maurice Burke. 2000. ‚ÄúOn Simulation and the\nTeaching of Statistics.‚Äù Teaching Statistics 22 (3):\n91‚Äì96. https://doi.org/10.1111/1467-9639.00033.\n\n\nMartins, Rui Manuel Da Costa. 2018. ‚ÄúLearning the Principles of\nSimulation Using the Birthday Problem.‚Äù Teaching\nStatistics 40 (3): 108‚Äì11. https://doi.org/10.1111/test.12164.\n\n\nMatthews, Robert, and Fiona Stones. 1998. ‚ÄúCoincidences: The Truth\nIs Out There.‚Äù Teaching Statistics 20 (1): 17‚Äì19.\nhttps://doi.org/https://doi.org/10.1111/j.1467-9639.1998.tb00752.x.\n\n\nThomas, F. H., and J. L. Moore. 1980. ‚ÄúCUSUM:\nComputer Simulation for Statistics Teaching.‚Äù Teaching\nStatistics 2 (1): 23‚Äì28. https://doi.org/10.1111/j.1467-9639.1980.tb00374.x.\n\n\nTintle, Nathan, Beth Chance, George Cobb, Soma Roy, Todd Swanson, and\nJill VanderStoep. 2015. ‚ÄúCombating Anti-Statistical Thinking Using\nSimulation-Based Methods Throughout the Undergraduate\nCurriculum.‚Äù The American Statistician 69 (4): 362‚Äì70.\nhttps://doi.org/10.1080/00031305.2015.1081619.\n\n\nZieffler, Andrew, and Joan B. Garfield. 2007. ‚ÄúStudying the Role\nof Simulation in Developing Students‚Äô Statistical Reasoning.‚Äù In\nProceedings of the 56th Session of the International Statistical\nInstitute (ISI). International Statistical Institute.",
    "crumbs": [
      "References"
    ]
  }
]