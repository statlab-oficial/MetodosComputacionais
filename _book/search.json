[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Métodos Computacionais",
    "section": "",
    "text": "Prefácio\nEste livro resulta de anos de experiência em sala de aula dos professores Ronald Targino, Rafael Braz, Juvêncio Nobre e Manoel Santos-Neto. Destina-se a apoiar os alunos da graduação em Estatística e do Programa de Pós-Graduação em Modelagem e Métodos Quantitativos (PPGMMQ) do Departamento de Estatística e Matemática Aplicada (DEMA) da Universidade Federal do Ceará (UFC).\nAo longo dos capítulos, abordamos a geração de números aleatórios (discretos e contínuos); métodos de suavização; simulação estocástica por inversão, rejeição e composição, bem como métodos de reamostragem; métodos de aproximação e integração; quadratura Gaussiana, integração de Monte Carlo e quadratura adaptativa; métodos de Monte Carlo em sentido amplo; amostradores MCMC, com ênfase em Gibbs e Metropolis–Hastings; otimização numérica via Newton–Raphson, Fisher scoring e quase-Newton, além do algoritmo EM; Bootstrap e Jackknife; diagnóstico de convergência; e aspectos computacionais em problemas práticos, com foco em implementação eficiente, estabilidade numérica e reprodutibilidade dos resultados.\nEsperamos que este material sirva não apenas como texto-base para as disciplinas Estatística Computacional (graduação em Estatística) e Métodos Computacionais em Estatística (Mestrado-PPGMMQ), mas também como suporte para aqueles que desejam programar com qualidade na área de Estatística.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "introducao.html",
    "href": "introducao.html",
    "title": "1  Introdução",
    "section": "",
    "text": "A simulação tem um papel preponderante na estatística moderna, e suas vantagens no ensino de Estatística são conhecidas há muito tempo. Em um de seus primeiros números, o periódico Teaching Statistics publicou artigos que aludem precisamente a isso. Thomas e Moore (1980) afirmaram que “a introdução do computador na sala de aula escolar trouxe uma nova técnica para o ensino, a técnica da simulação”. Zieffler e Garfield (2007) e Tintle et al. (2015) discutem o papel e a importância da aprendizagem baseada em simulação no currículo de graduação em Estatística. No entanto, outros autores (por exemplo, Hodgson e Burke 2000) discutem alguns problemas que podem surgir ao ensinar uma disciplina por meio de simulação, a saber, o desenvolvimento de certos equívocos na mente dos estudantes (Martins 2018).\n\n\n\n\nHodgson, Ted, e Maurice Burke. 2000. “On Simulation and the Teaching of Statistics”. Teaching Statistics 22 (3): 91–96. https://doi.org/10.1111/1467-9639.00033.\n\n\nMartins, Rui Manuel Da Costa. 2018. “Learning the Principles of Simulation Using the Birthday Problem”. Teaching Statistics 40 (3): 108–11. https://doi.org/10.1111/test.12164.\n\n\nThomas, F. H., e J. L. Moore. 1980. “CUSUM: Computer Simulation for Statistics Teaching”. Teaching Statistics 2 (1): 23–28. https://doi.org/10.1111/j.1467-9639.1980.tb00374.x.\n\n\nTintle, Nathan, Beth Chance, George Cobb, Soma Roy, Todd Swanson, e Jill VanderStoep. 2015. “Combating Anti-Statistical Thinking Using Simulation-Based Methods Throughout the Undergraduate Curriculum”. The American Statistician 69 (4): 362–70. https://doi.org/10.1080/00031305.2015.1081619.\n\n\nZieffler, Andrew, e Joan B. Garfield. 2007. “Studying the Role of Simulation in Developing Students’ Statistical Reasoning”. Em Proceedings of the 56th Session of the International Statistical Institute (ISI). International Statistical Institute.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Motivação",
    "section": "",
    "text": "2.1 Da teoria à simulação\nDo ponto de vista teórico, a probabilidade de que todos os aniversários sejam distintos entre \\(r\\) pessoas é\n\\[\\Pr(\\text{todos distintos}) \\;=\\; \\prod_{i=1}^{r-1} \\frac{365-i}{365}\n\\;=\\; \\left(1 - \\frac{1}{365}\\right)\\!\\left(1 - \\frac{2}{365}\\right)\\!\\cdots\\!\\left(1 - \\frac{r-1}{365}\\right).\\]\nLogo, a probabilidade de pelo menos uma coincidência é\n\\[p_r = 1 - \\Pr(\\text{todos distintos}).\\]\nEsse produto é conceitualmente claro, mas fica pouco manejável mentalmente para \\(k\\) moderados. É aqui que a simulação computacional pode entrar como aliada didática e científica.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Motivação</span>"
    ]
  },
  {
    "objectID": "intro.html#um-atalho-analítico-útil",
    "href": "intro.html#um-atalho-analítico-útil",
    "title": "2  Motivação",
    "section": "2.2 Um atalho analítico útil",
    "text": "2.2 Um atalho analítico útil\nO produto acima admite uma aproximação exponencial simples e acurada, obtida tomando logaritmo e usando a expansão para argumentos pequenos:\n\\[\\ln(1 - x) = -x + o(x), \\quad (x \\to 0). \\] Aplicando ao produto,\n\\[\n\\begin{aligned}\n\\ln\\!\\big(1 - p_r\\big)\n\\;&=\\;\n\\sum_{i=1}^{r-1} \\ln\\!\\left(1 - \\frac{i}{365}\\right)\\\\\n\\;&\\approx\\;\n-\\,\\sum_{i=1}^{r-1} \\frac{i}{365}\n\\;=\\;\n-\\,\\frac{1 + 2 + \\cdots + (r-1)}{365}\n\\;=\\;\n-\\,\\frac{r(r-1)}{2 \\cdot 365}.\\end{aligned}\\]\nExponentiando e isolando \\(p_r\\), obtemos a aproximação\n\\[\np_r \\;\\approx\\; 1 - \\exp\\!\\left\\{-\\,\\frac{r(r-1)}{730}\\right\\}.\n\\]\nEssa fórmula tem três virtudes didáticas:\n\nClareza: exibe explicitamente o papel do número de pares \\(\\binom{r}{2}\\).\nRapidez: permite cálculos aproximados para valores de \\(r\\) de interesse.\nBoas aproximações já para \\(r\\) na casa de dezenas.\n\n\n\n\n\n\n\nExemplo Rápido\n\n\n\n\nPara 23 pessoas:\n\n\\[p_{23}^{(\\text{aprox})}\n  \\;=\\;\n  1 - \\exp\\!\\left\\{-\\frac{23\\cdot22}{730}\\right\\}\n  \\;=\\;\n  1 - \\exp\\!\\{-0.69315\\}\n  \\;\\approx\\; 0.500,\\]\nalinhando-se ao resultado clássico de que 23 pessoas já superam 50% de chance de coincidência.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Motivação</span>"
    ]
  },
  {
    "objectID": "intro.html#o-papel-da-simulação",
    "href": "intro.html#o-papel-da-simulação",
    "title": "2  Motivação",
    "section": "2.3 O papel da simulação",
    "text": "2.3 O papel da simulação\nA simulação estatística permite reproduzir o experimento de forma empírica: sorteamos aleatoriamente dias de aniversário para os indivíduos e verificamos se há repetições. Repetindo o processo milhares de vezes, obtemos uma estimativa para a probabilidade de coincidência.\nPor exemplo, em R:\n\nk &lt;- 23\nbirthdays &lt;- sample(1:365, k, replace = TRUE)\nany(duplicated(birthdays))\n\n[1] FALSE\n\n\nAo repetir esse procedimento muitas vezes (por exemplo, 10.000 simulações), podemos estimar a proporção de conjuntos com coincidência. Pela Lei dos Grandes Números, essa estimativa converge para o valor teórico de aproximadamente 0,507 quando \\(k=23\\).\n\nset.seed(123) #reprodutibilidade\n\nk &lt;- 23\nB &lt;- 10000\n\nacertos &lt;- 0L\ni &lt;- 0L\n\nrepeat {\n  i &lt;- i + 1L\n  bdays &lt;- sample(1:365, k, replace = TRUE)\n  acertos &lt;- acertos + as.integer(any(duplicated(bdays)))\n  if (i &gt;= B) break\n}\n\np_hat &lt;- acertos / B\np_hat\n\n[1] 0.5073",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Motivação</span>"
    ]
  },
  {
    "objectID": "intro.html#atividade-problema-do-aniversário-22-jogadores",
    "href": "intro.html#atividade-problema-do-aniversário-22-jogadores",
    "title": "2  Motivação",
    "section": "2.4 Atividade: Problema do Aniversário (22 jogadores)",
    "text": "2.4 Atividade: Problema do Aniversário (22 jogadores)\nNesta motivação consideramos um exemplo discutido em Martins (2018) que é o conhecido e amplamente divulgado problema do aniversário (ver, por exemplo, Falk 2014). Martins (2018) segue o exemplo de Matthews e Stones (1998), considerando duas equipes de futebol e, portanto, coincidências de aniversário entre 22 jogadores. Martins (2018) afirma que um resultado positivo importante dessa atividade é a discussão que surgirá naturalmente entre os estudantes, com o professor atuando como mediador. Além disso, os estudantes adoram jogos e a descoberta prática, e a simulação facilita o engajamento nessas atividades, ao mesmo tempo que ilustra resultados que podem ser não intuitivos, bem como teoria geral, como a Lei dos Grandes Números.\nAgora iremos considerar o seguinte problema:\nO problema: Em uma partida de futebol, qual é a probabilidade de que pelo menos dois dos 22 jogadores façam aniversário no mesmo dia?\nEm um pais chamado de país do futebol, o contexto é proposital: o futebol é popular e as probabilidades resultantes são contraintuitivas. Antes de qualquer cálculo, considere as hipóteses: (i) todos os 365 dias do ano são igualmente prováveis para qualquer aniversário; (ii) as datas de aniversário dos jogadores são independentes entre si.\nObjetivos\n\nEstimar, via simulação, a probabilidade de coincidência de aniversários.\n\nRelacionar frequência relativa, Lei dos Grandes Números e variação amostral.\n\nComparar o resultado exato e aproximado.\n\n\nHipóteses\n\n365 dias equiprováveis, datas independentes, ignorar bissexto/gêmeos.\n\nMateriais\n\nR (ou Posit Cloud), roteiro com comandos sample(), table(), mean().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Motivação</span>"
    ]
  },
  {
    "objectID": "intro.html#exercícios",
    "href": "intro.html#exercícios",
    "title": "2  Motivação",
    "section": "2.5 Exercícios",
    "text": "2.5 Exercícios\n\nDeterminar o menor número de pessoas que deve estar em uma sala para que se possa apostar, com mais de 50% de chance de ganhar, que entre elas existam pelo menos duas com o mesmo aniversário.\nDeterminar o menor número de outras pessoas que deve estar em uma sala com você para que se possa apostar, com mais de 50% de chance de ganhar, que pelo menos uma delas tenha o mesmo aniversário que o seu.\n\n\n\n\n\nFalk, Ruma. 2014. “A Closer Look at the Notorious Birthday Coincidences”. Teaching Statistics 36 (2): 41–46. https://doi.org/10.1111/test.12014.\n\n\nMartins, Rui Manuel Da Costa. 2018. “Learning the Principles of Simulation Using the Birthday Problem”. Teaching Statistics 40 (3): 108–11. https://doi.org/10.1111/test.12164.\n\n\nMatthews, Robert, e Fiona Stones. 1998. “Coincidences: the truth is out there”. Teaching Statistics 20 (1): 17–19. https://doi.org/https://doi.org/10.1111/j.1467-9639.1998.tb00752.x.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Motivação</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Números Uniformes",
    "section": "",
    "text": "3.1 Geração de sequências \\(U(0,1)\\)\nUma abordagem é utilizar dispositivos físicos aleatorizadores, como máquinas que sorteiam números de loteria, roletas ou circuitos eletrônicos que produzem “ruído aleatório”. Contudo, tais dispositivos apresentam desvantagens:\nUma forma simples de obter reprodutibilidade é armazenar a sequência em um dispositivo de memória (HD, CD-ROM, livro). De fato, a RAND Corporation publicou A Million Random Digits with 100 000 Random Normal Deviates (1955). Entretanto, acessar armazenamento externo milhares ou milhões de vezes torna a simulação lenta.\nAssim, a abordagem preferida é gerar números pseudoaleatórios em tempo de execução, via recorrências determinísticas sobre inteiros. Isso permite:\nEntretanto, a escolha inadequada da recorrência pode gerar sequências com baixa qualidade estatística.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Números Uniformes</span>"
    ]
  },
  {
    "objectID": "summary.html#geração-de-sequências-u01",
    "href": "summary.html#geração-de-sequências-u01",
    "title": "3  Números Uniformes",
    "section": "",
    "text": "Baixa velocidade e dificuldade de integração direta com computadores.\nNecessidade de reprodutibilidade da sequência. Por exemplo, para verificação de código ou comparação de políticas em um modelo de simulação, usando a mesma sequência para reduzir a variância da diferença entre resultados.\n\n\n\n\nGeração rápida;\nEliminação do problema de armazenamento;\nReprodutibilidade controlada.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Números Uniformes</span>"
    ]
  },
  {
    "objectID": "summary.html#geradores-congruenciais-lineares",
    "href": "summary.html#geradores-congruenciais-lineares",
    "title": "3  Números Uniformes",
    "section": "3.2 Geradores Congruenciais Lineares",
    "text": "3.2 Geradores Congruenciais Lineares\nUm Gerador Congruencial Linear (LGC) produz uma sequência de inteiros não negativos \\(X_i\\), \\(i = 1, 2, \\ldots\\), por meio da relação de recorrência:\n\\[X_i = (a X_{i-1} + c) \\bmod m, \\quad i = 1, 2, \\ldots,\\] em que \\(a &gt; 0\\) é o multiplicador, \\(X_0 \\ge 0\\) é a semente (seed), \\(c \\ge 0\\) é o incremento e \\(m &gt; 0\\) é o módulo.\nOs valores \\(a, c, X_0\\) estão no intervalo \\([0, m-1]\\). O número pseudoaleatório \\(R_i\\) é obtido por:\n\\[R_i = \\frac{X_i}{m}, \\quad R_i \\in (0,1).\\]\nSe \\(m\\) for suficientemente grande, os valores discretos \\(0/m, 1/m, \\ldots, (m-1)/m\\) são tão próximos que \\(R_i\\) pode ser tratado como variável contínua.\n\n3.2.1 Exemplo\nSeja o gerador:\n\\[X_i = (9 X_{i-1} + 3) \\bmod 24, \\quad i \\geq 1.\\]\nEscolhendo \\(X_0 = 3\\):\n\\[X_1 = (9 \\times 3 + 3) \\bmod 24 = 14\\]\n\\[X_2 = (9 \\times 14 + 3) \\bmod 24 = 1\\]\ne assim por diante.\nA sequência \\(R_i = X_i / 16\\) gerada terá período \\(\\ell = 16\\).\n\n\n3.2.2 Implementação em R\n\n# Função LCG genérica\nlcg &lt;- function(a, c, m, seed, n) {\n  x &lt;- numeric(n)\n  x[1] &lt;- seed\n  for (i in 2:n) {\n    x[i] &lt;- (a * x[i-1] + c) %% m\n  }\n  r &lt;- x / m\n  return(list(X = x, R = r))\n}\n\n# Exemplo com a = 9, c = 3, m = 24, seed = 3\nresultado &lt;- lcg(a = 9, c = 3, m = 24, seed = 3, n = 20)\nresultado$X\n\n [1]  3  6  9 12 15 18 21  0  3  6  9 12 15 18 21  0  3  6  9 12\n\nresultado$R\n\n [1] 0.125 0.250 0.375 0.500 0.625 0.750 0.875 0.000 0.125 0.250 0.375 0.500\n[13] 0.625 0.750 0.875 0.000 0.125 0.250 0.375 0.500",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Números Uniformes</span>"
    ]
  },
  {
    "objectID": "summary.html#geradores-congruenciais-lineares-mistos",
    "href": "summary.html#geradores-congruenciais-lineares-mistos",
    "title": "3  Números Uniformes",
    "section": "3.3 Geradores Congruenciais Lineares Mistos",
    "text": "3.3 Geradores Congruenciais Lineares Mistos\nNos LCGs mistos temos \\(c&gt;0\\). Uma escolha prática é \\(m = 2^b\\), onde \\(b\\) é o número de bits utilizável para inteiros positivos na arquitetura/linguagem. Em muitos ambientes, inteiros usam 32 bits (um para o sinal), implicando \\(b=31\\) e intervalo \\([-2^{31}, 2^{31}-1]\\).\nQuando \\(m=2^b\\), obtemos período completo (\\(\\ell=m\\)) se:\n\n\\(c\\) é ímpar (garante \\(\\gcd(c,m)=1\\));\n\n\\(a-1\\) é múltiplo de todos os fatores primos de \\(m\\) e também de 4 (como \\(m\\) é potência de 2).\n\nEssa é a razão de geradores simples com \\(m=2^b\\), \\(c\\) ímpar e \\(a \\equiv 1 \\pmod 4\\) atingirem \\(\\ell=m\\).\n\n3.3.1 Questão de estouro e aritmética modular\nEm linguagens com inteiros limitados, calcular \\(aX_{i-1}+c\\) pode transbordar. Soluções comuns:\n\nusar precisão estendida (64 bits) ou bibliotecas de inteiros grandes;\nempregar truques de aritmética modular (como o método de Schrage) para evitar overflow;\ntrabalhar com módulo \\(m=2^b\\) e aproveitar o “wrap” de bits.\n\nA seguir, implementamos LCG misto com \\(m=2^{31}\\), \\(a=906185749\\), \\(c=1\\). Parâmetros com boas propriedades estatísticas relatadas na literatura.\n\n\n3.3.2 Implementação em R (com segurança de overflow)\nPara garantir a correção do módulo com inteiros grandes, usaremos bit64 (inteiros de 64 bits) e normalizaremos para \\((0,1)\\).\n\n#if (!requireNamespace(\"bit64\", quietly = TRUE)) {\n#  install.packages(\"bit64\")\n#}\n\nlibrary(bit64)\n\nlcg_misto &lt;- function(n, seed = 3456L,\n                      a = 906185749L,\n                      c = 1L,\n                      m = bit64::as.integer64(2)^31) {\n  # Trabalha em integer64 para evitar perda de precisão\n  x &lt;- bit64::as.integer64(seed)\n  outX &lt;- bit64::integer64(n)\n  outR &lt;- numeric(n)\n  outX[1] &lt;- x\n  outR[1] &lt;- as.double(x) / as.double(m)\n  for (i in 2:n) {\n    x &lt;- (bit64::as.integer64(a) * x + bit64::as.integer64(c)) %% m\n    outX[i] &lt;- x\n    outR[i] &lt;- as.double(x) / as.double(m)\n  }\n  list(X = outX, R = outR)\n}\n\n# Exemplo: primeiros 5 números com seed = 3456\nset.seed(NULL)\ng1 &lt;- lcg_misto(n = 5, seed = 3456L)\ng1$X\n\ninteger64\n[1] 3456       746789761  460230038  1591485775 1024426876\n\ng1$R\n\n[1] 1.609325e-06 3.477511e-01 2.143113e-01 7.410933e-01 4.770359e-01",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Números Uniformes</span>"
    ]
  },
  {
    "objectID": "summary.html#geradores-congruenciais-lineares-multiplicativos",
    "href": "summary.html#geradores-congruenciais-lineares-multiplicativos",
    "title": "3  Números Uniformes",
    "section": "3.4 Geradores Congruenciais Lineares Multiplicativos",
    "text": "3.4 Geradores Congruenciais Lineares Multiplicativos\nNo caso multiplicativo, temos \\(c = 0\\), e a recorrência fica:\n\\[\nX_i = (a X_{i-1}) \\bmod m\n\\]\n\n3.4.1 Características e restrições\n\nSe \\(X_i = 0\\) em algum passo, toda a sequência futura será zero — portanto \\(X_0 \\neq 0\\).\nSe \\(a = 1\\), a sequência é constante — também deve ser evitado.\nO período máximo possível é \\(m - 1\\), e ele só é atingido quando:\n\n\\(m\\) é primo;\n\\(a\\) é uma raiz primitiva módulo \\(m\\).\n\n\n\n\n3.4.2 Definição de raiz primitiva\nUm número \\(a\\) é raiz primitiva módulo \\(m\\) se seus poderes geram todos os inteiros não nulos módulo \\(m\\).\nMatematicamente, \\(a\\) satisfaz:\n\\[\nm \\nmid a^{(m-1)/q} - 1, \\quad \\forall q \\text{ primo que divide } m-1\n\\]\nEsse tipo de gerador é chamado Gerador de Módulo Primo e Período Máximo.\n\n\n\n3.4.3 Exemplo de implementação em R\nA seguir, implementamos um gerador multiplicativo com módulo primo \\(m = 2^{31} - 1\\) (primo de Mersenne) e multiplicador \\(a = 630360016\\), conhecido por apresentar boas propriedades estatísticas.\n\nif (!requireNamespace(\"gmp\", quietly = TRUE)) {\n  install.packages(\"gmp\")\n}\nlibrary(gmp)\n\nlcg_mult_primo &lt;- function(n, seed, a = 630360016, m = 2147483647) {\n  A &lt;- as.bigz(a); M &lt;- as.bigz(m)\n  x &lt;- as.bigz(seed)\n  X &lt;- integer(n); R &lt;- numeric(n)\n  for (i in seq_len(n)) {\n    X[i] &lt;- as.integer(x)    \n    R[i] &lt;- as.numeric(x) / m    \n    x &lt;- (A * x) %% M            \n  }\n  list(X = X, R = R)\n}\n\n# Exemplo: gerar 10 valores\ng2 &lt;- lcg_mult_primo(n = 10, seed = 12345L)\ng2$X\n\n [1]      12345 1461144439 1646755962  423395703 2041926374  720397004\n [7]  140279311  597861375  629442282  759842328\n\ng2$R\n\n [1] 5.748589e-06 6.803984e-01 7.668305e-01 1.971590e-01 9.508461e-01\n [6] 3.354610e-01 6.532264e-02 2.784009e-01 2.931069e-01 3.538292e-01",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Números Uniformes</span>"
    ]
  },
  {
    "objectID": "pseudo.html",
    "href": "pseudo.html",
    "title": "4  Número Pseudoaleatórios",
    "section": "",
    "text": "4.1 Introdução",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Número Pseudoaleatórios</span>"
    ]
  },
  {
    "objectID": "pseudo.html#métodos-para-geração-de-variáveil-aleatórias-discretas",
    "href": "pseudo.html#métodos-para-geração-de-variáveil-aleatórias-discretas",
    "title": "4  Número Pseudoaleatórios",
    "section": "4.2 Métodos para Geração de Variáveil Aleatórias Discretas",
    "text": "4.2 Métodos para Geração de Variáveil Aleatórias Discretas\n\n4.2.1 Método da transformação inversa\n\n\n4.2.2 Método da Aceitação-Rejeição\n\n\n4.2.3 Método da Composição",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Número Pseudoaleatórios</span>"
    ]
  },
  {
    "objectID": "pseudo.html#métodos-para-geração-de-variáveil-aleatórias-contínuas",
    "href": "pseudo.html#métodos-para-geração-de-variáveil-aleatórias-contínuas",
    "title": "4  Número Pseudoaleatórios",
    "section": "4.3 Métodos para Geração de Variáveil Aleatórias Contínuas",
    "text": "4.3 Métodos para Geração de Variáveil Aleatórias Contínuas\n\n4.3.1 Método da transformação inversa\n\n\n4.3.2 Método da Aceitação-Rejeição",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Número Pseudoaleatórios</span>"
    ]
  },
  {
    "objectID": "otimizaca.html",
    "href": "otimizaca.html",
    "title": "5  Otimização Numérica",
    "section": "",
    "text": "5.1 Método de Newton",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Otimização Numérica</span>"
    ]
  },
  {
    "objectID": "otimizaca.html#método-de-newton-raphson",
    "href": "otimizaca.html#método-de-newton-raphson",
    "title": "5  Otimização Numérica",
    "section": "5.2 Método de Newton-Raphson",
    "text": "5.2 Método de Newton-Raphson",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Otimização Numérica</span>"
    ]
  },
  {
    "objectID": "otimizaca.html#método-escore-de-fisher",
    "href": "otimizaca.html#método-escore-de-fisher",
    "title": "5  Otimização Numérica",
    "section": "5.3 Método Escore de Fisher",
    "text": "5.3 Método Escore de Fisher",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Otimização Numérica</span>"
    ]
  },
  {
    "objectID": "otimizaca.html#método-bfgs",
    "href": "otimizaca.html#método-bfgs",
    "title": "5  Otimização Numérica",
    "section": "5.4 Método BFGS",
    "text": "5.4 Método BFGS",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Otimização Numérica</span>"
    ]
  },
  {
    "objectID": "boot.html",
    "href": "boot.html",
    "title": "6  Métodos de Reamostragem",
    "section": "",
    "text": "6.1 Bootstrap",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Métodos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#bootstrap",
    "href": "boot.html#bootstrap",
    "title": "6  Métodos de Reamostragem",
    "section": "",
    "text": "6.1.1 Introdução\nBootstrap é um método (computacional) de reamostragem baseado em subamostras de uma amostra observada, sendo introduzido por Efron (1979). Pode ser utilizado com o propósito de estimar erros padrão, viés de estimadores, construir intervalos de confiança, testes de hipóteses, entre outros. Pode ser utilizando sob duas abordagens: paramétrica e não paramétrica. A abordagem paramétrica exige um modelo estatística, enquanto que na abordagem não paramétrica não há suposição de modelo estatístico; toma-se por base uma distribuição empírica que atribui probabilidade \\(1/n\\) para cada um dos \\(n\\) elementos da amostra. Algumas referências importantes neste tema são: Efron (1979), Wu (1986), Fisher \\(\\&\\) Hall (1989), Fredman (1986), Efron \\(\\&\\) Tibshirani (1993), Horowitz (1997), Davison \\(\\&\\) Hinkley (1997).\n\n\n6.1.2 Acurária da média amostral\nExperimento com 16 ratos, divididos em dois grupos: um grupo recebeu o tratamento e um outro grupo não recebeu o tratamento (controle). O tempo de sobrevivência (dias) é apresentado para cada um dos ratos. O tratamento prolonga a vida?\n\n\n\n\n\n\n\n\n\n\n\n\ngrupo\ntempo 1\ntempo 2\ntempo 3\nn\nmédia\n\\(\\widehat{ep}\\)(média)\n\n\n\n\ntratamento (X)\n94\n197\n16\n7\n86,86\n25,24\n\n\n\n38\n99\n141\n\n\n\n\n\n\n23\n\n\n\n\n\n\n\ncontrole (Y)\n52\n104\n146\n9\n56,22\n14,14\n\n\n\n10\n51\n30\n\n\n\n\n\n\n40\n27\n46\n\n\n\n\n\ndiferença\n\n\n\n\n30,63\n28,93\n\n\n\nAlguns pontos importantes são:\n\nA resposta à pergunta dependerá de quão acurado(s) é(são) o(s) estimador(es).\nO erro padrão é uma medida (muito usual) de acurácia de estimador.\nerro padrão estimado para a média amostral \\(\\bar{X}\\) \\[\\widehat{ep}(\\bar{X})=\\sqrt{\\dfrac{s^2}{n}},\\] em que \\(s^2=\\sum_{i=1}^n(x_i-\\bar{x})^2/(n-1)\\).\nerro padrão de qualquer estimador é definido pela raiz quadrada de sua variância\n\nAlém disso, temos que:\n\nerro padrão pequeno \\(\\longrightarrow\\) acurácia alta\nerro padrão grande \\(\\longrightarrow\\) acurácia baixa\nacurácia alta (baixa) indica que o estimador apresenta valores próximos (distantes) ao seu valor esperado\nespera-se que 68\\(\\%\\) dos valores do estimador estejam a menos de um erro padrão do seu valor esperado, e 95\\(\\%\\), a menos de dois erros padrões\nerro padrão da diferença \\((\\bar{X}-\\bar{Y})\\): \\[28.93=\\sqrt{25.24^2+14.14^2}.\\]\n\nResposta à pergunta: a diferença observada 30.63 é somente 30.63/28.93=1.05 erros padrões (estimados) maior que zero, indicando um resultado não significativo, ou seja, o tratamento não aumenta o tempo médio de vida (considerando a teoria dos testes de hipóteses).\nO erro padrão para o estimador média amostral apresenta fórmula conhecida, mas há casos em que não dispomos de fórmulas. Suponha que haja interesse em comparar os dois grupos de ratos em relação aos tempos medianos. Temos: md(X)=94 e md(Y)=46. A diferença é 48, maior que a diferença para as médias. Com base na mediana, o tratamento prolonga a vida?\n\n\n6.1.3 Estimativa bootstrap do erro padrão\nConsiderações:\n\n\\(\\underset{\\sim}{x} = (x_1,\\,x_2\\,\\ldots,\\,x_n)\\): vetor de dados observado (amostra original de tamanho \\(n\\))\n\\(s(\\underset{\\sim}{x})\\): estatística de interesse (por exemplo, média amostral)\nUma amostra bootstrap \\(\\underset{\\sim}{x}^*=(x_1^*, x_2^*, \\ldots, x_n^*)\\) é obtida pela amostragem aleatória de tamanho \\(n\\), com reposição, de \\(\\underset{\\sim}{x}\\). Por exemplo, com \\(n=7\\), poderíamos obter \\(\\underset{\\sim}{x}^*=(x_1^*, x_2^*, \\ldots, x_n^*)=(x_5, x_7, x_5, x_4, x_7, x_3, x_1)\\).\n\nO algoritmo bootstrap\n\ngerar \\(B\\) amostras bootstrap independentes: \\(\\underset{\\sim}{x}^{*^{1}}, \\underset{\\sim}{x}^{*^{2}}, \\ldots, \\underset{\\sim}{x}^{*^{B}},\\) cada uma de tamanho \\(n\\) e \\(50 \\leq B \\leq 200\\).\ncalcular \\(s(\\underset{\\sim}{x}^{*^{b}})\\), \\(b=1, 2, \\ldots, B\\). \\(s(\\underset{\\sim}{x}^{*^{b}})\\) é denominada réplica bootstrap de \\(s(\\underset{\\sim}{x})\\)\ncalcular \\(\\hat{ep}_{boot}=\\hat{ep}_{B}=\\sqrt{ \\dfrac{\\sum_{b=1}^{B}[s(\\underset{\\sim}{x}^{*^{b}})-s(.)]^2}{B-1}},\\)em que \\(s(.)=\\frac{\\sum_{b=1}^{B}s(\\underset{\\sim}{x}^{*^{b}})}{B}\\)\n\nSintaxe do R para calcular a estimativa bootstrap do erro padrão da média do tempo de sobrevida dos ratos do grupo tratamento.\n\nset.seed(1234)\n# grupo tratamento (amostra original)\nx &lt;- c(94, 197, 16, 38, 99, 141, 23) \nn &lt;- length(x)\ns &lt;- 0  # estatística de interesse\nB &lt;- 50 # no. de amostras bootstrap \n\nfor(i in 1:B){\n   # réplica bootstrap para o estimador média\n   s[i] &lt;- mean(sample(x,n,replace=TRUE)) \n }\n# estimativa bootstrap para o erro padrão da média\nep &lt;-  sd(s); ep\n\n[1] 27.41873\n\n\nEstimativas bootstrap do erro padrão da média e da mediana do tempo de sobrevivência dos ratos do grupo tratamento. A mediana é menos acurada (erros padrões maiores) que a média para esse conjunto de dados.\n\n\n\n\\(B\\)\n50\n100\n250\n500\n1000\n\\(\\infty\\)\n\n\n\n\nmédia\n19.72\n23.63\n22.32\n23.79\n23.02\n23.36\n\n\nmediana\n32.21\n36.35\n34.46\n36.72\n36.48\n37.83\n\n\n\nFormalização:\n\n\\(\\underset{\\sim}{X} = X_1, X_2,\\dots, X_n\\): amostra aleatória de uma f.d.a. \\(F\\)\n\\(\\underset{\\sim}{x} = (x_1, x_2,\\ldots, x_n)\\): amostra aleatória observada de \\(F\\)\n\\(\\Theta=t(F)\\): parâmetro (\\(\\Theta\\) uma função de \\(F\\))\n\\(\\hat{\\Theta}=s(\\underset{\\sim}{X})\\): estimador para \\(\\Theta\\)\n\\(\\hat{\\theta}=s(\\underset{\\sim}{x})\\): estimativa para \\(\\Theta\\)\nQuão acurado é o estimador \\(\\hat{\\Theta}\\)?\nSeja \\(\\hat{F}\\) a distribuição empírica que atribui a probabilidade \\(1/n\\) para cada valor observado \\(x_i\\), \\(i=1,2,\\ldots, n\\). A amostra bootstrap \\(\\underset{\\sim}{x}^*\\) é definida como a amostra aleatória com reposição de tamanho \\(n\\) extraída de \\(\\hat{F}\\). \\[\\underset{\\sim}{x}^*=(x_1^*,\\,x_2^*\\,\\ldots,\\,x_n^*)\\] \\[\\hat{F}\\longrightarrow (x_1^*,\\,x_2^*\\,\\ldots,\\,x_n^*)\\] Nota:\n\\(\\underset{\\sim}{x}^*\\): o símbolo \\(*\\) indica que a amostra não é a original \\(\\underset{\\sim}{x}\\), mas uma versão aleatorizada(reamostrada) de \\(\\underset{\\sim}{x}\\)\na cada amostra bootstrap corresponde uma réplica bootstrap de \\(\\hat\\theta\\), \\(\\hat\\theta^* =s(\\underset{\\sim}{x}^*)\\)\n\\(ep_F(\\hat\\Theta)\\) é estimado por \\(ep_{\\hat{F}}(\\hat\\Theta^*)\\), chamado para o erro padrão \\(\\hat\\Theta\\)\nRaramente faz-se necessário \\(B\\geq 200\\) para estimar erro padrão; valores (muito) maiores são necessários, por exemplo, para IC bootstrap.\n\\(\\displaystyle\\lim_{B \\to \\infty} \\hat{ep}_B=ep_{\\hat{F}}=ep_{\\hat{F}}(\\hat\\Theta^*)\\)\nO estimador bootstrap ideal \\(ep_{\\hat{F}}(\\hat\\Theta^*)\\) e sua aproximação \\(\\hat{ep}_B\\) são chamados estimadores bootstrap não-paramétricos, pois baseiam-se em \\(\\hat{F}\\), o estimador não-paramétrico de \\(F\\).\ntotal de amostras bootstrap distintas (combinação com repetição): \\[\\binom{2n -1}{n}.\\]\nNo R: ver as funções factorial(), choose(), combn(), combinations().\n\n\n6.1.3.1 Exemplo\nDados de faculdades americanas de direito. População: \\(N=82\\) faculdades. Amostra aleatória: \\(n=15\\) faculdades. Variáveis analisadas: LSAT (escore médio em um teste), GPA (pontuação média na faculdade).\n\n\n\nescola\nLSAT\nGPA\nescola\nLSAT\nGPA\n\n\n\n\n1\n576\n3,39\n9\n651\n3,36\n\n\n2\n635\n3,30\n10\n605\n3,13\n\n\n3\n558\n2,81\n11\n653\n3,12\n\n\n4\n578\n3,03\n12\n575\n2,74\n\n\n5\n666\n3,44\n13\n545\n2,76\n\n\n6\n580\n3,07\n14\n572\n2,88\n\n\n7\n555\n3,00\n15\n594\n2,96\n\n\n8\n661\n3,43\n\n\n\n\n\n\n\nFaçamos \\(Y\\)=LSAT e \\(Z\\)=GPA. A estatística (estimador) de interesse é o coeficiente de correlação amostral entre as variáveis \\(Y\\) e \\(Z\\): \\[\\hat{\\Theta}=corr(Y,Z)=\\dfrac{Cov(Y,Z)}{DP(Y).DP(Z)}=\\dfrac{\\sum_{i=1}^n(Y_i-\\bar{Y})(Z_i-\\bar{Z})/n}{DP(Y).DP(Z)}\\]\nPara os dados observados, a estimativa do coeficiente de correlação amostral é 0.776. Quão acurado é o estimador?\n\n\nEstimativas bootstrap do erro padrão para \\(\\hat{\\Theta} = corr(Y,Z)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B\\)\n25\n50\n100\n200\n400\n800\n1600\n3200\n\n\n\n\n\\(\\hat{ep}_B\\)\n0,140\n0,142\n0,151\n0,143\n0,141\n0,137\n0,133\n0,132\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nNo caso de valores extremos inflacionarem fortemente \\(\\hat{ep}_B\\), uma medida mais robusta para o estimador bootstrap do erro padrão é desejável. (ver Efron \\(\\&\\) Tibshirani (1993)).\n\n\n\n\n\n\n\n\nNota\n\n\n\nInferências baseadas na distribuição normal são “questionáveis” quando o histograma das réplicas bootstrap indica forte assimetria.\n\n\n\n\n6.1.3.2 Exercícios:\n\nConsidere \\(B = 3200\\). Para cada uma das 3200 amostras bootstrap, obtenha a réplica bootstrap \\(\\hat{\\theta}^*=corr(y^*,z^*)\\). Faça o histograma das réplicas.\nA Tabela 3.2, p. 21 do livro texto, apresenta os dados populacionais das 82 faculdades. Selecione 3200 amostras aleatórias de tamanho \\(n = 15\\). Para cada uma dessas amostra calcule o coeficiente de correlação e faça o histograma.\n\n\n\n\n6.1.4 Bootstrap Paramétrico\nO estimador bootstrap paramétrico do erro padrão é definido por \\[ep_{\\hat{F}_{par}}(\\hat\\Theta^*),\\] em que \\(\\hat{F}_{par}\\) é um estimador de \\(F\\) derivado do modelo paramétrico para os dados.\nPara os dados das faculdades: Vamos supor que a população (LSAT, GPA) possa ser descrita por um modelo paramétrico normal bivariado \\(F\\). Estimamos \\(F\\) por \\(\\hat{F}_{normal}\\), que denota a f.d.a. de uma normal bivariada com vetor de médias e matriz de covariâncias \\((\\bar{y},\\bar{z})\\) e \\(\\dfrac{1}{14}\\left(\n\\begin{array}{ll}\n\\sum(y_i-\\bar{y})^2            &  \\sum(y_i-\\bar{y})(z_i-\\bar{z}) \\\\\n\\sum(y_i-\\bar{y})(z_i-\\bar{z}) &  \\sum(z_i-\\bar{z})^2  \\\\\n\\end{array}\n\\right)\\).\nO estimador bootstrap paramétrico do erro padrão da correlação \\(\\hat{\\Theta}\\) será dado por \\(ep_{\\hat{F}_{normal}}(\\hat\\Theta^*)\\). Esse estimador bootstrap ideal será aproximado por \\(\\hat{ep}_{B}\\) (conforme algoritmo a seguir).\nO algoritmo bootstrap:\n\nextrair \\(B\\) amostras de tamanho \\(n\\) de \\(\\hat{F}_{par}\\): \\(\\underset{\\sim}{x}^{*^{1}}, \\underset{\\sim}{x}^{*^{2}}, \\ldots, \\underset{\\sim}{x}^{*^{B}}\\)\ncalcular \\(s(\\underset{\\sim}{x}^{*^{b}})\\), \\(b=1, 2, \\ldots, B\\). \\(s(\\underset{\\sim}{x}^{*^{b}})\\) é a réplica bootstrap de \\(s(\\underset{\\sim}{x})\\)\ncalcular \\(\\hat{ep}_{boot}=\\hat{ep}_{B}=\\sqrt{ \\dfrac{\\sum_{b=1}^{B}[s(\\underset{\\sim}{x}^{*^{b}})-s(.)]^2}{B-1}},\\)em que \\(s(.)=\\frac{\\sum_{b=1}^{B}s(\\underset{\\sim}{x}^{*^{b}})}{B}\\)\n\nNo exemplo das faculdades, assumindo o modelo normal bivariado, extraímos \\(B\\) amostras de tamanho \\(n=15\\) de \\(\\hat{F}_{normal}\\), calculamos o coeficiente de correlação para cada amostra e, por fim, calculamos o desvio padrão desses coeficientes de correlação. Usando \\(B=3200\\) encontramos \\(\\hat{ep}_{B}=0.124\\), que é próximo ao valor 0.131 obtido com o bootstrap não-paramétrico.\nA fórmula teórica para o erro padrão do coeficiente de correlação é \\(\\dfrac{1-\\hat{\\Theta}^2}{\\sqrt{n-3}}\\), com \\(\\hat{\\Theta}=corr(Y,Z)\\). Vimos que \\(\\hat{\\theta}=0.776\\), o que resulta a estimativa 0.115 para o erro padrão do coeficiente de correlação entre as variáveis \\(Y\\)=LSAT e \\(Z\\)=GPA.\nTransformação de Fisher para o coeficiente de correlação \\(\\hat{\\Theta}\\): \\(\\hat{\\zeta}=0.5\\log\\left(\\dfrac{1+\\hat{\\Theta}}{1-\\hat{\\Theta}}\\right)\\). Assim, \\(\\hat{\\zeta}\\) tem distribuição aproximadamente normal com média \\(0.5\\log\\left(\\dfrac{1+\\Theta}{1-\\Theta}\\right)\\) e variância \\(\\dfrac{1}{n-3}\\). O erro padrão para \\(\\hat{\\zeta}\\) é \\(\\sqrt{\\dfrac{1}{n-3}}\\). Para o exemplo das faculdades, o valor é \\(\\dfrac{1}{\\sqrt{12}}=0.289\\).\nA título de comparação com o procedimento bootstrap, a estatística (estimador) \\(\\hat{\\zeta}\\) foi estimado em cada uma das \\(B=3200\\) amostras bootstrap. O desvio padrão das réplicas bootstrap resultou 0.290 (muito próximo ao valor teórico 0.289). Histogramas para as correlações \\(\\hat{\\theta}^*\\)e para os \\(\\hat{\\zeta}^*\\).\n\n\n\n\n\n\nImportante\n\n\n\nMuitas fórmulas para os erros padrões são aproximações baseadas na teoria normal e isso “explica” os resultados próximos obtidos com o uso do bootstrap paramétrico que extrai amostras a partir da distribuição normal.\n\n\nVantagens do bootstrap sobre os métodos tradicionais:\n\nbootstrap não-paramétrico: não é necessário fazer suposições de modelos paramétricos para a população;\nbootstrap paramétrico: possibilita estimar erros padrões em problemas para os quais não há fórmulas para os erros padrões.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Métodos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#jackknife",
    "href": "boot.html#jackknife",
    "title": "6  Métodos de Reamostragem",
    "section": "6.2 Jackknife",
    "text": "6.2 Jackknife\n\n6.2.1 Introdução\nJackknife é uma técnica para estimar viés e erro padrão de estimadores. É uma técnica que antecede o bootstrap e foi proposta no trabalho pioneiro Quenouille (1949) para reduzir viés do estimador da correlação serial.\nFormalização:\n\n\\(\\underset{\\sim}{X} = (X_1, X_2, \\dots, X_n)\\): amostra aleatória de uma f.d.a. \\(F\\)\n\\(\\underset{\\sim}{x} = (x_1, x_2, \\dots, x_n)\\): amostra aleatória observada de \\(F\\)\n\\(\\Theta=t(F)\\): parâmetro (\\(\\Theta\\) uma função de \\(F\\))\n\\(\\hat{\\Theta}=s(\\underset{\\sim}{X})\\): estimador para \\(\\Theta\\)\n\\(\\hat{\\theta}=s(\\underset{\\sim}{x})\\): estimativa para \\(\\Theta\\)\n\nO jackknife toma como base \\(n\\) amostras de tamanho \\(n-1\\) selecionadas da amostra aleatória observada. A \\(i\\)-ésima amostra jackknife consiste da amostra observada com a \\(i\\)-ésima observação removida, \\(i=1,2, \\ldots, n\\): \\[{\\underset{\\sim}{x}}_{(i)}=(x_1,x_2,\\ldots, x_{i-1},x_{i+1}, \\ldots, x_n).\\]\n\n\n6.2.2 Estimador do viés\nPara cada amostra \\(i\\) jackknife, é obtida a réplica jackknife \\(\\hat{\\theta}_{(i)}=s({\\underset{\\sim}{x}}_{(i)})\\) e o estimador jackknife do viés de \\(\\hat{\\Theta}\\) é dado por: \\[\\widehat{\\mbox{viés}}_{jack} =(n-1)( \\hat{\\Theta}_{(\\cdot)} - \\hat{\\Theta} ),\\] em que \\(\\hat{\\Theta}_{(\\cdot)}=\\sum_{i=1}^n\\dfrac{\\hat{\\Theta}_{(i)}}{n}\\).\n\n\n6.2.3 Estimado do erro padrão\nO estimador jackknife do erro padrão de \\(\\hat{\\Theta}\\) pode ser escrito da seguinte maneira: \\[\\widehat{ep}_{jack} =\\left[ \\dfrac{n-1}{n}\\sum_{i=1}^n (\\hat{\\Theta}_{(i)} - \\hat{\\Theta}_{(\\cdot)})^2\\right]^{\\frac{1}{2}},\\] em que \\(\\hat{\\Theta}_{(\\cdot)}=\\sum_{i=1}^n\\dfrac{\\hat{\\Theta}_{(i)}}{n}\\).\nAlgumas considerações importantes:\n\nBootstrap: amostragem aleatória com reposição;\nJackknife: amostras fixas;\nJackknife requer o cálculo do estimador apenas para \\(n\\) amostras;\nA acurácia do estimador jackknife do erro padrão depende de quão próximo o estimador é da linearidade. Para funções fortemente não lineares, o jackknife pode ser ineficiente;\nEstimador linear: \\(\\hat{\\Theta}=s(\\underset{\\sim}{X})=\\mu +\\frac{1}{n}\\sum_{i=1}^n\\alpha(X_i).\\)\n\n\n6.2.3.1 Exemplo\nConsidere a amostra observada: \\(\\underset{\\sim}{x}=(10,26,30,40,48)\\) e o estimador: mediana(\\(\\hat{\\Theta}\\)). As amostras e réplicas jackknife são dadas, respectivamente, por:\n\n\\({\\underset{\\sim}{x}}_{(1)}=(26,30,40,48)\\) e \\(\\hat{\\theta}_{(1)}=35;\\)\n\\({\\underset{\\sim}{x}}_{(2)}=(10,30,40,48)\\) e \\(\\hat{\\theta}_{(2)}=35;\\)\n\\({\\underset{\\sim}{x}}_{(3)}=(10,26,40,48)\\) e \\(\\hat{\\theta}_{(3)}=33;\\)\n\\({\\underset{\\sim}{x}}_{(4)}=(10,26,30,48)\\) e \\(\\hat{\\theta}_{(4)}=28;\\)\n\\({\\underset{\\sim}{x}}_{(5)}=(10,26,30,40)\\) e \\(\\hat{\\theta}_{(5)}=28.\\)\n\nDesta forma, a estimativa jackknife do erro padrão de \\(\\hat{\\Theta}\\): \\[\\widehat{ep}_{jack} =\\left[ \\dfrac{4}{5}\\sum_{i=1}^5 (\\hat{\\theta}_{(i)} - \\hat{\\theta}_{(\\cdot)})^2\\right]^{\\frac{1}{2}}\\approx 6.38,\\] com \\(\\hat{\\theta}_{(\\cdot)}=\\sum_{i=1}^5\\dfrac{\\hat{\\theta}_{(i)}}{5}=31.8.\\)\nA seguir, a sintaxe do R para calcular a estimativa jackknife do erro padrão para a mediana.\n\nx &lt;- c(10,26,30,40,48)\nn &lt;- length(x)\n\nest.jack &lt;- 0\n\nfor(i in 1:n){ \n  est.jack[i] &lt;- median(x[-i])\n}\n\nep.jack &lt;- sqrt(((n-1)^2/n)*var(est.jack))\nep.jack\n\n[1] 6.374951",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Métodos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "boot.html#intervalos-de-confiança",
    "href": "boot.html#intervalos-de-confiança",
    "title": "6  Métodos de Reamostragem",
    "section": "6.3 Intervalos de Confiança",
    "text": "6.3 Intervalos de Confiança\n\n6.3.1 Intervalo de Confiança Normal e t-Student\n\n\n6.3.2 Intervalo de Confiança bootstrap-t\n\n\n6.3.3 Intervalos de Confiança bootstrap percentil\n\n\n6.3.4 Intervalos de Confiança bootstrap - versões aprimoradas",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Métodos de Reamostragem</span>"
    ]
  },
  {
    "objectID": "mc.html",
    "href": "mc.html",
    "title": "7  Métodos de Monte Carlo",
    "section": "",
    "text": "7.1 Introdução",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Métodos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#integração-de-monte-carlo",
    "href": "mc.html#integração-de-monte-carlo",
    "title": "7  Métodos de Monte Carlo",
    "section": "7.2 Integração de Monte Carlo",
    "text": "7.2 Integração de Monte Carlo",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Métodos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#erro-de-monte-carlo",
    "href": "mc.html#erro-de-monte-carlo",
    "title": "7  Métodos de Monte Carlo",
    "section": "7.3 Erro de Monte Carlo",
    "text": "7.3 Erro de Monte Carlo",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Métodos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#monte-carlo-via-função-de-importância",
    "href": "mc.html#monte-carlo-via-função-de-importância",
    "title": "7  Métodos de Monte Carlo",
    "section": "7.4 Monte Carlo via Função de Importância",
    "text": "7.4 Monte Carlo via Função de Importância",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Métodos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "mc.html#método-de-máxima-verossimilhança",
    "href": "mc.html#método-de-máxima-verossimilhança",
    "title": "7  Métodos de Monte Carlo",
    "section": "7.5 Método de Máxima Verossimilhança",
    "text": "7.5 Método de Máxima Verossimilhança",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Métodos de Monte Carlo</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Falk, Ruma. 2014. “A Closer Look at the Notorious Birthday\nCoincidences.” Teaching Statistics 36 (2): 41–46. https://doi.org/10.1111/test.12014.\n\n\nHodgson, Ted, and Maurice Burke. 2000. “On Simulation and the\nTeaching of Statistics.” Teaching Statistics 22 (3):\n91–96. https://doi.org/10.1111/1467-9639.00033.\n\n\nMartins, Rui Manuel Da Costa. 2018. “Learning the Principles of\nSimulation Using the Birthday Problem.” Teaching\nStatistics 40 (3): 108–11. https://doi.org/10.1111/test.12164.\n\n\nMatthews, Robert, and Fiona Stones. 1998. “Coincidences: The Truth\nIs Out There.” Teaching Statistics 20 (1): 17–19.\nhttps://doi.org/https://doi.org/10.1111/j.1467-9639.1998.tb00752.x.\n\n\nThomas, F. H., and J. L. Moore. 1980. “CUSUM:\nComputer Simulation for Statistics Teaching.” Teaching\nStatistics 2 (1): 23–28. https://doi.org/10.1111/j.1467-9639.1980.tb00374.x.\n\n\nTintle, Nathan, Beth Chance, George Cobb, Soma Roy, Todd Swanson, and\nJill VanderStoep. 2015. “Combating Anti-Statistical Thinking Using\nSimulation-Based Methods Throughout the Undergraduate\nCurriculum.” The American Statistician 69 (4): 362–70.\nhttps://doi.org/10.1080/00031305.2015.1081619.\n\n\nZieffler, Andrew, and Joan B. Garfield. 2007. “Studying the Role\nof Simulation in Developing Students’ Statistical Reasoning.” In\nProceedings of the 56th Session of the International Statistical\nInstitute (ISI). International Statistical Institute.",
    "crumbs": [
      "References"
    ]
  }
]